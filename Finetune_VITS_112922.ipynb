{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishweshwar133/Ecommerce/blob/main/Finetune_VITS_112922.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_HkOd4jwqIb"
      },
      "source": [
        "**Fine tune a VITS model with the Coqui TTS framework using audio samples of your choice.**\n",
        "\n",
        "Thank you to all of the [Coqui TTS](https://https://github.com/coqui-ai/TTS) contributors, and [@Thorsten-Voice](https://https://www.youtube.com/c/ThorstenMueller ) (I think he wrote this original script). I have no idea who wrote the rnnoise loop, but they are great, too. I just smash things together. -nn\n",
        "\n",
        "If you have run the script before, and want to examine your training session using Tensorboard, skip to the bottom section and run cells A-C, and then the Tensorboard cells."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Run this cell to connect your Google Drive account to save files."
      ],
      "metadata": {
        "id": "nOoSyCc8h4WI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RUb7_iJu1mb",
        "outputId": "c6b83773-ec5b-4c51-ad6a-97c0748cf907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIvwEf9zydzN"
      },
      "source": [
        "2. Download and Build Rnnoise and Requirements (Optional if not processing .wav files for a new dataset. Multiple passes of rnnoise over samples may degrade audio quality. You should also preview your output dataset before training to ensure that the processing has not degraded any of your samples.  Occasionally, it will denoise too much of the tail end of phrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HigwgxZxEXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f23e48-7f71-4408-e801-2e9ff7dd2a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyloudnorm\n",
            "  Downloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from pyloudnorm) (0.18.3)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from pyloudnorm) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pyloudnorm) (1.10.1)\n",
            "Installing collected packages: pyloudnorm\n",
            "Successfully installed pyloudnorm-0.1.1\n",
            "Cloning into 'rnnoise'...\n",
            "remote: Enumerating objects: 420, done.\u001b[K\n",
            "remote: Counting objects: 100% (219/219), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 420 (delta 194), reused 192 (delta 192), pack-reused 201\u001b[K\n",
            "Receiving objects: 100% (420/420), 836.04 KiB | 7.40 MiB/s, done.\n",
            "Resolving deltas: 100% (221/221), done.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'python-dev-is-python2' instead of 'python-dev'\n",
            "autoconf is already the newest version (2.69-11.1).\n",
            "autoconf set to manually installed.\n",
            "automake is already the newest version (1:1.16.1-4ubuntu6).\n",
            "automake set to manually installed.\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu4).\n",
            "curl is already the newest version (7.68.0-1ubuntu2.18).\n",
            "The following additional packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libpython2-dev libpython2-stdlib\n",
            "  libpython2.7 libpython2.7-dev libsox-fmt-alsa libsox-fmt-base libsox3\n",
            "  python-is-python2 python2 python2-dev python2-minimal python2.7-dev\n",
            "Suggested packages:\n",
            "  libsox-fmt-all libtool-doc gcj-jdk python2-doc python-tk\n",
            "The following NEW packages will be installed:\n",
            "  libopencore-amrnb0 libopencore-amrwb0 libpython2-dev libpython2-stdlib\n",
            "  libpython2.7 libpython2.7-dev libsox-fmt-alsa libsox-fmt-base libsox3\n",
            "  libtool python-dev-is-python2 python-is-python2 python2 python2-dev\n",
            "  python2-minimal python2.7-dev sox\n",
            "0 upgraded, 17 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 4,543 kB of archives.\n",
            "After this operation, 20.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-minimal amd64 2.7.17-2ubuntu4 [27.5 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-stdlib amd64 2.7.17-2ubuntu4 [7,072 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2 amd64 2.7.17-2ubuntu4 [26.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7 amd64 2.7.18-1~20.04.3 [1,037 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libpython2.7-dev amd64 2.7.18-1~20.04.3 [2,466 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-dev amd64 2.7.17-2ubuntu4 [7,140 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [225 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [10.5 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [31.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libtool all 2.4.6-14 [161 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-is-python2 all 2.7.17-4 [2,496 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python2.7-dev amd64 2.7.18-1~20.04.3 [293 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-dev amd64 2.7.17-2ubuntu4 [1,268 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-dev-is-python2 all 2.7.17-4 [1,396 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 sox amd64 14.4.2+git20190427-2+deb11u2build0.20.04.1 [102 kB]\n",
            "Fetched 4,543 kB in 2s (2,008 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 17.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python2-minimal.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../python2-minimal_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libpython2-stdlib:amd64.\n",
            "Preparing to unpack .../libpython2-stdlib_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package python2.\n",
            "(Reading database ... 122547 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python2_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2 (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../01-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../02-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libpython2.7:amd64.\n",
            "Preparing to unpack .../03-libpython2.7_2.7.18-1~20.04.3_amd64.deb ...\n",
            "Unpacking libpython2.7:amd64 (2.7.18-1~20.04.3) ...\n",
            "Selecting previously unselected package libpython2.7-dev:amd64.\n",
            "Preparing to unpack .../04-libpython2.7-dev_2.7.18-1~20.04.3_amd64.deb ...\n",
            "Unpacking libpython2.7-dev:amd64 (2.7.18-1~20.04.3) ...\n",
            "Selecting previously unselected package libpython2-dev:amd64.\n",
            "Preparing to unpack .../05-libpython2-dev_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking libpython2-dev:amd64 (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../06-libsox3_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../07-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../08-libsox-fmt-base_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Selecting previously unselected package libtool.\n",
            "Preparing to unpack .../09-libtool_2.4.6-14_all.deb ...\n",
            "Unpacking libtool (2.4.6-14) ...\n",
            "Selecting previously unselected package python-is-python2.\n",
            "Preparing to unpack .../10-python-is-python2_2.7.17-4_all.deb ...\n",
            "Unpacking python-is-python2 (2.7.17-4) ...\n",
            "Selecting previously unselected package python2.7-dev.\n",
            "Preparing to unpack .../11-python2.7-dev_2.7.18-1~20.04.3_amd64.deb ...\n",
            "Unpacking python2.7-dev (2.7.18-1~20.04.3) ...\n",
            "Selecting previously unselected package python2-dev.\n",
            "Preparing to unpack .../12-python2-dev_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2-dev (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package python-dev-is-python2.\n",
            "Preparing to unpack .../13-python-dev-is-python2_2.7.17-4_all.deb ...\n",
            "Unpacking python-dev-is-python2 (2.7.17-4) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../14-sox_14.4.2+git20190427-2+deb11u2build0.20.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up libpython2.7:amd64 (2.7.18-1~20.04.3) ...\n",
            "Setting up libpython2.7-dev:amd64 (2.7.18-1~20.04.3) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up libtool (2.4.6-14) ...\n",
            "Setting up libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2 (2.7.17-2ubuntu4) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libpython2-dev:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Setting up python-is-python2 (2.7.17-4) ...\n",
            "Setting up python2.7-dev (2.7.18-1~20.04.3) ...\n",
            "Setting up python2-dev (2.7.17-2ubuntu4) ...\n",
            "Setting up python-dev-is-python2 (2.7.17-4) ...\n",
            "Setting up sox (14.4.2+git20190427-2+deb11u2build0.20.04.1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "/content/rnnoise\n",
            "Updating build configuration files for rnnoise, please wait....\n",
            "libtoolize: putting auxiliary files in '.'.\n",
            "libtoolize: linking file './ltmain.sh'\n",
            "libtoolize: putting macros in AC_CONFIG_MACRO_DIRS, 'm4'.\n",
            "libtoolize: linking file 'm4/libtool.m4'\n",
            "libtoolize: linking file 'm4/ltoptions.m4'\n",
            "libtoolize: linking file 'm4/ltsugar.m4'\n",
            "libtoolize: linking file 'm4/ltversion.m4'\n",
            "libtoolize: linking file 'm4/lt~obsolete.m4'\n",
            "configure.ac:19: installing './compile'\n",
            "configure.ac:27: installing './config.guess'\n",
            "configure.ac:27: installing './config.sub'\n",
            "configure.ac:22: installing './install-sh'\n",
            "configure.ac:22: installing './missing'\n",
            "Makefile.am: installing './depcomp'\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking whether gcc understands -c and -o together... yes\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for grep that handles long lines and -e... /usr/bin/grep\n",
            "checking for egrep... /usr/bin/grep -E\n",
            "checking for ANSI C header files... yes\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking minix/config.h usability... no\n",
            "checking minix/config.h presence... no\n",
            "checking for minix/config.h... no\n",
            "checking whether it is safe to define __EXTENSIONS__... yes\n",
            "checking for special C compiler options needed for large files... no\n",
            "checking for _FILE_OFFSET_BITS value needed for large files... no\n",
            "checking for a BSD-compatible install... /usr/bin/install -c\n",
            "checking whether build environment is sane... yes\n",
            "checking for a thread-safe mkdir -p... /usr/bin/mkdir -p\n",
            "checking for gawk... no\n",
            "checking for mawk... mawk\n",
            "checking whether make sets $(MAKE)... yes\n",
            "checking whether make supports the include directive... yes (GNU style)\n",
            "checking whether make supports nested variables... yes\n",
            "checking dependency style of gcc... gcc3\n",
            "checking whether to enable maintainer-specific portions of Makefiles... yes\n",
            "checking for inline... inline\n",
            "checking build system type... x86_64-pc-linux-gnu\n",
            "checking host system type... x86_64-pc-linux-gnu\n",
            "checking how to print strings... printf\n",
            "checking for a sed that does not truncate output... /usr/bin/sed\n",
            "checking for fgrep... /usr/bin/grep -F\n",
            "checking for ld used by gcc... /usr/bin/ld\n",
            "checking if the linker (/usr/bin/ld) is GNU ld... yes\n",
            "checking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\n",
            "checking the name lister (/usr/bin/nm -B) interface... BSD nm\n",
            "checking whether ln -s works... yes\n",
            "checking the maximum length of command line arguments... 1572864\n",
            "checking how to convert x86_64-pc-linux-gnu file names to x86_64-pc-linux-gnu format... func_convert_file_noop\n",
            "checking how to convert x86_64-pc-linux-gnu file names to toolchain format... func_convert_file_noop\n",
            "checking for /usr/bin/ld option to reload object files... -r\n",
            "checking for objdump... objdump\n",
            "checking how to recognize dependent libraries... pass_all\n",
            "checking for dlltool... no\n",
            "checking how to associate runtime and link libraries... printf %s\\n\n",
            "checking for ar... ar\n",
            "checking for archiver @FILE support... @\n",
            "checking for strip... strip\n",
            "checking for ranlib... ranlib\n",
            "checking command to parse /usr/bin/nm -B output from gcc object... ok\n",
            "checking for sysroot... no\n",
            "checking for a working dd... /usr/bin/dd\n",
            "checking how to truncate binary pipes... /usr/bin/dd bs=4096 count=1\n",
            "checking for mt... no\n",
            "checking if : is a manifest tool... no\n",
            "checking for dlfcn.h... yes\n",
            "checking for objdir... .libs\n",
            "checking if gcc supports -fno-rtti -fno-exceptions... no\n",
            "checking for gcc option to produce PIC... -fPIC -DPIC\n",
            "checking if gcc PIC flag -fPIC -DPIC works... yes\n",
            "checking if gcc static flag -static works... yes\n",
            "checking if gcc supports -c -o file.o... yes\n",
            "checking if gcc supports -c -o file.o... (cached) yes\n",
            "checking whether the gcc linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\n",
            "checking whether -lc should be explicitly linked in... no\n",
            "checking dynamic linker characteristics... GNU/Linux ld.so\n",
            "checking how to hardcode library paths into programs... immediate\n",
            "checking whether stripping libraries is possible... yes\n",
            "checking if libtool supports shared libraries... yes\n",
            "checking whether to build shared libraries... yes\n",
            "checking whether to build static libraries... yes\n",
            "checking whether make supports nested variables... (cached) yes\n",
            "checking if gcc supports -pedantic flag... yes\n",
            "checking if gcc supports -Wall flag... yes\n",
            "checking if gcc supports -Wextra flag... yes\n",
            "checking if gcc supports -Wno-sign-compare flag... yes\n",
            "checking if gcc supports -Wno-parentheses flag... yes\n",
            "checking if gcc supports -Wno-long-long flag... yes\n",
            "checking for cos in -lm... yes\n",
            "checking for gcc way to treat warnings as errors... -Werror\n",
            "checking if gcc supports __attribute__(( visibility(\"default\") ))... yes\n",
            "checking if gcc supports -fvisibility=hidden... yes\n",
            "checking for doxygen... no\n",
            "checking for dot... yes\n",
            "checking that generated files are newer than configure... done\n",
            "configure: creating ./config.status\n",
            "config.status: creating Makefile\n",
            "config.status: creating rnnoise.pc\n",
            "config.status: creating rnnoise-uninstalled.pc\n",
            "config.status: creating doc/Doxyfile\n",
            "config.status: creating config.h\n",
            "config.status: executing depfiles commands\n",
            "config.status: executing libtool commands\n",
            "configure:\n",
            "------------------------------------------------------------------------\n",
            "  rnnoise unknown: Automatic configuration OK.\n",
            "\n",
            "    Assertions ................... no\n",
            "\n",
            "    Hidden visibility ............ yes\n",
            "\n",
            "    API code examples ............ yes\n",
            "    API documentation ............ yes\n",
            "------------------------------------------------------------------------\n",
            "\n",
            "test -z \"librnnoise.la\" || rm -f librnnoise.la\n",
            "rm -f ./so_locations\n",
            "rm -rf .libs _libs\n",
            "rm -rf examples/.libs examples/_libs\n",
            "rm -rf src/.libs src/_libs\n",
            " rm -f examples/rnnoise_demo\n",
            "rm -f *.o\n",
            "rm -f examples/*.o\n",
            "rm -f src/*.o\n",
            "rm -f src/*.lo\n",
            "rm -f *.lo\n",
            "make  all-am\n",
            "make[1]: Entering directory '/content/rnnoise'\n",
            "  CC       examples/rnnoise_demo.o\n",
            "\u001b[01m\u001b[Kexamples/rnnoise_demo.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/rnnoise_demo.c:48:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "   48 |     \u001b[01;35m\u001b[Kfread(tmp, sizeof(short), FRAME_SIZE, f1)\u001b[m\u001b[K;\n",
            "      |     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "  CC       src/denoise.lo\n",
            "  CC       src/rnn.lo\n",
            "  CC       src/rnn_data.lo\n",
            "  CC       src/rnn_reader.lo\n",
            "  CC       src/pitch.lo\n",
            "  CC       src/kiss_fft.lo\n",
            "  CC       src/celt_lpc.lo\n",
            "  CCLD     librnnoise.la\n",
            "  CCLD     examples/rnnoise_demo\n",
            "make[1]: Leaving directory '/content/rnnoise'\n"
          ]
        }
      ],
      "source": [
        "!pip install pyloudnorm\n",
        "!git clone https://github.com/xiph/rnnoise.git\n",
        "!sudo apt-get install curl autoconf automake libtool python-dev pkg-config sox\n",
        "%cd /content/rnnoise\n",
        "!sh autogen.sh\n",
        "!sh configure\n",
        "!make clean\n",
        "!make"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWzQveIIy7QW"
      },
      "source": [
        "3. Install Coqui TTS, espeak-ng phonemeizer, download Coqui TTS source and examples from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyCWXW_2y_nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f207aefd-9508-496d-9c74-3e26d2a490a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "The following NEW packages will be installed:\n",
            "  espeak-ng espeak-ng-data libespeak-ng1 libpcaudio0 libsonic0\n",
            "0 upgraded, 5 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 4,215 kB of archives.\n",
            "After this operation, 12.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libpcaudio0 amd64 1.1-4 [7,908 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libsonic0 amd64 0.2.0-8 [13.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 espeak-ng-data amd64 1.50+dfsg-6 [3,682 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libespeak-ng1 amd64 1.50+dfsg-6 [189 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 espeak-ng amd64 1.50+dfsg-6 [322 kB]\n",
            "Fetched 4,215 kB in 1s (3,483 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpcaudio0:amd64.\n",
            "(Reading database ... 122811 files and directories currently installed.)\n",
            "Preparing to unpack .../libpcaudio0_1.1-4_amd64.deb ...\n",
            "Unpacking libpcaudio0:amd64 (1.1-4) ...\n",
            "Selecting previously unselected package libsonic0:amd64.\n",
            "Preparing to unpack .../libsonic0_0.2.0-8_amd64.deb ...\n",
            "Unpacking libsonic0:amd64 (0.2.0-8) ...\n",
            "Selecting previously unselected package espeak-ng-data:amd64.\n",
            "Preparing to unpack .../espeak-ng-data_1.50+dfsg-6_amd64.deb ...\n",
            "Unpacking espeak-ng-data:amd64 (1.50+dfsg-6) ...\n",
            "Selecting previously unselected package libespeak-ng1:amd64.\n",
            "Preparing to unpack .../libespeak-ng1_1.50+dfsg-6_amd64.deb ...\n",
            "Unpacking libespeak-ng1:amd64 (1.50+dfsg-6) ...\n",
            "Selecting previously unselected package espeak-ng.\n",
            "Preparing to unpack .../espeak-ng_1.50+dfsg-6_amd64.deb ...\n",
            "Unpacking espeak-ng (1.50+dfsg-6) ...\n",
            "Setting up libpcaudio0:amd64 (1.1-4) ...\n",
            "Setting up libsonic0:amd64 (0.2.0-8) ...\n",
            "Setting up espeak-ng-data:amd64 (1.50+dfsg-6) ...\n",
            "Setting up libespeak-ng1:amd64 (1.50+dfsg-6) ...\n",
            "Setting up espeak-ng (1.50+dfsg-6) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Cloning into 'TTS'...\n",
            "remote: Enumerating objects: 29730, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 29730 (delta 74), reused 86 (delta 58), pack-reused 29607\u001b[K\n",
            "Receiving objects: 100% (29730/29730), 159.14 MiB | 28.37 MiB/s, done.\n",
            "Resolving deltas: 100% (21554/21554), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting TTS\n",
            "  Downloading TTS-0.13.3-cp310-cp310-manylinux1_x86_64.whl (655 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m655.3/655.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut[de]==2.2.3\n",
            "  Downloading gruut-2.2.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: librosa==0.10.0.* in /usr/local/lib/python3.10/dist-packages (from TTS) (0.10.0.post2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from TTS) (3.8.1)\n",
            "Collecting g2pkk>=0.1.1\n",
            "  Downloading g2pkk-0.1.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numba==0.56.4 in /usr/local/lib/python3.10/dist-packages (from TTS) (0.56.4)\n",
            "Collecting pysbd\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from TTS) (2.2.4)\n",
            "Collecting unidic-lite==1.0.8\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bangla==0.0.2\n",
            "  Downloading bangla-0.0.2-py2.py3-none-any.whl (6.2 kB)\n",
            "Collecting bnunicodenormalizer==0.1.1\n",
            "  Downloading bnunicodenormalizer-0.1.1.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mecab-python3==1.0.5\n",
            "  Downloading mecab_python3-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.1/581.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from TTS) (0.12.1)\n",
            "Collecting coqpit>=0.0.16\n",
            "  Downloading coqpit-0.0.17-py3-none-any.whl (13 kB)\n",
            "Collecting inflect==5.6.0\n",
            "  Downloading inflect-5.6.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from TTS) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from TTS) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from TTS) (1.5.3)\n",
            "Collecting cython==0.29.28\n",
            "  Downloading Cython-0.29.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting umap-learn==0.5.1\n",
            "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pypinyin\n",
            "  Downloading pypinyin-0.48.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bnnumerizer\n",
            "  Downloading bnnumerizer-0.0.2.tar.gz (4.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from TTS) (2.0.0+cu118)\n",
            "Collecting trainer==0.0.20\n",
            "  Downloading trainer-0.0.20-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from TTS) (3.7.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from TTS) (6.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from TTS) (2.0.1+cu118)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from TTS) (0.42.1)\n",
            "Requirement already satisfied: fsspec>=2021.04.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (2023.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from TTS) (4.65.0)\n",
            "Collecting jamo\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from TTS) (1.10.1)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from gruut[de]==2.2.3->TTS) (2.12.1)\n",
            "Collecting dateparser~=1.1.0\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut-ipa<1.0,>=0.12.0\n",
            "  Downloading gruut-ipa-0.13.0.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gruut_lang_en~=2.0.0\n",
            "  Downloading gruut_lang_en-2.0.0.tar.gz (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonlines~=1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting networkx<3.0.0,>=2.5.0\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting num2words<1.0.0,>=0.5.10\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite~=0.9.7\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gruut_lang_de~=2.0.0\n",
            "  Downloading gruut_lang_de-2.0.0.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (0.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.2.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (0.3.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (3.0.0)\n",
            "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.10.0.*->TTS) (1.6.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba==0.56.4->TTS) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba==0.56.4->TTS) (67.7.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from trainer==0.0.20->TTS) (5.9.5)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->TTS) (1.15.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (3.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->TTS) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->TTS) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->TTS) (3.25.2)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->TTS) (23.1.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->TTS) (8.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->TTS) (2.1.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->TTS) (2.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->TTS) (2.8.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->TTS) (2022.10.31)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->TTS) (2022.7.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->TTS) (2.21)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (4.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->TTS) (2.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jsonlines~=1.2.0->gruut[de]==2.2.3->TTS) (1.16.0)\n",
            "Collecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (2.27.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa==0.10.0.*->TTS) (3.1.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp->TTS) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->TTS) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa==0.10.0.*->TTS) (2022.12.7)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal->dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal->dateparser~=1.1.0->gruut[de]==2.2.3->TTS) (2023.3)\n",
            "Building wheels for collected packages: bnunicodenormalizer, umap-learn, unidic-lite, bnnumerizer, gruut-ipa, gruut_lang_de, gruut_lang_en, pynndescent, gruut, docopt\n",
            "  Building wheel for bnunicodenormalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnunicodenormalizer: filename=bnunicodenormalizer-0.1.1-py3-none-any.whl size=21912 sha256=f1f86ed10ae4e5e17e69292368e1b3d7bd4a13d7534808f67506a84a5beaac51\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/f6/01/9e68ecec7c7ea85fc9431cfac42eba1c5a5f6debe5070de5c7\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.1-py3-none-any.whl size=76564 sha256=12603db1a2ead8a2f4d10732eef45393633dacda6a56a1b4af65dc50ab1765ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/21/8e/802cb9c4c606a67139f538cb17bf3bf1b98b739a7900469953\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658834 sha256=3ccaed31679a20f99c228a2ef76336bec40dea411b7658d8dfafe6f4e2bd0b3e\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bnnumerizer: filename=bnnumerizer-0.0.2-py3-none-any.whl size=5276 sha256=822a337a6e10fa3259080aadf233f1fba6e59f44ed2ab832d803151db3553dda\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/6b/e8/223172e7d5c9f72df3ea1a0d9258f3a8ab5b28e827728edef5\n",
            "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut-ipa: filename=gruut_ipa-0.13.0-py3-none-any.whl size=104891 sha256=de4e0a0922e9a6080e33bd5b00d58f5c9b74d3ad6a6d4339066e7aad0f159d1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/18/49/e4f500ecdf0babe757953f844e4d7cd1ea81c5503c09bfe984\n",
            "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_de: filename=gruut_lang_de-2.0.0-py3-none-any.whl size=18498199 sha256=e678e3625bfe845c1fb41c7bc70e6aaf7152d74ec8d259172bc50e18c9e3fbb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/9a/05/cfce98f0c41a1a540f15708c4a02df190b82d84cf91ef6bc7f\n",
            "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut_lang_en: filename=gruut_lang_en-2.0.0-py3-none-any.whl size=15297195 sha256=9d876f095bfa14a2d60229f5334a9a89bd8f52973b864919f8c6b01aca620518\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/9c/fb/77c655a9fbd78cdb9935d0ab65d80ddd0a3bcf7dbe18261650\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55640 sha256=5b0ee748e74503a664445566bc29f06fa747b47eabc3582e45ffb99dad7b8e03\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gruut: filename=gruut-2.2.3-py3-none-any.whl size=75817 sha256=908a5567e68c1d64d2bb0c93c4a492ec94465aac74c11f1714f182e7fbbc9d74\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/57/a8/f9de532daf5214f53644f20f3a9e6f69269453c87df9c0a817\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=f5171a92a0212e91c8104207cd2acf50b9a5f041f5eabbd7b9f623ad9dc013c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built bnunicodenormalizer umap-learn unidic-lite bnnumerizer gruut-ipa gruut_lang_de gruut_lang_en pynndescent gruut docopt\n",
            "Installing collected packages: unidic-lite, python-crfsuite, mecab-python3, jamo, gruut_lang_en, gruut_lang_de, docopt, bnunicodenormalizer, bnnumerizer, bangla, pysbd, pypinyin, protobuf, num2words, networkx, multidict, jsonlines, inflect, gruut-ipa, frozenlist, cython, coqpit, async-timeout, anyascii, yarl, tensorboardX, g2pkk, aiosignal, pynndescent, dateparser, aiohttp, umap-learn, gruut, trainer, TTS\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.1\n",
            "    Uninstalling networkx-3.1:\n",
            "      Successfully uninstalled networkx-3.1\n",
            "  Attempting uninstall: inflect\n",
            "    Found existing installation: inflect 6.0.4\n",
            "    Uninstalling inflect-6.0.4:\n",
            "      Successfully uninstalled inflect-6.0.4\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 0.29.34\n",
            "    Uninstalling Cython-0.29.34:\n",
            "      Successfully uninstalled Cython-0.29.34\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed TTS-0.13.3 aiohttp-3.8.4 aiosignal-1.3.1 anyascii-0.3.2 async-timeout-4.0.2 bangla-0.0.2 bnnumerizer-0.0.2 bnunicodenormalizer-0.1.1 coqpit-0.0.17 cython-0.29.28 dateparser-1.1.8 docopt-0.6.2 frozenlist-1.3.3 g2pkk-0.1.2 gruut-2.2.3 gruut-ipa-0.13.0 gruut_lang_de-2.0.0 gruut_lang_en-2.0.0 inflect-5.6.0 jamo-0.4.1 jsonlines-1.2.0 mecab-python3-1.0.5 multidict-6.0.4 networkx-2.8.8 num2words-0.5.12 protobuf-3.19.6 pynndescent-0.5.10 pypinyin-0.48.0 pysbd-0.3.4 python-crfsuite-0.9.9 tensorboardX-2.6 trainer-0.0.20 umap-learn-0.5.1 unidic-lite-1.0.8 yarl-1.9.2\n",
            "No API token found for 🐸Coqui Studio voices - https://coqui.ai \n",
            "Visit 🔗https://app.coqui.ai/account to get one.\n",
            "Set it as an environment variable `export COQUI_STUDIO_TOKEN=<token>`\n",
            "\n",
            " Name format: type/language/dataset/model\n",
            " 1: tts_models/multilingual/multi-dataset/your_tts\n",
            " 2: tts_models/bg/cv/vits\n",
            " 3: tts_models/cs/cv/vits\n",
            " 4: tts_models/da/cv/vits\n",
            " 5: tts_models/et/cv/vits\n",
            " 6: tts_models/ga/cv/vits\n",
            " 7: tts_models/en/ek1/tacotron2\n",
            " 8: tts_models/en/ljspeech/tacotron2-DDC\n",
            " 9: tts_models/en/ljspeech/tacotron2-DDC_ph\n",
            " 10: tts_models/en/ljspeech/glow-tts\n",
            " 11: tts_models/en/ljspeech/speedy-speech\n",
            " 12: tts_models/en/ljspeech/tacotron2-DCA\n",
            " 13: tts_models/en/ljspeech/vits\n",
            " 14: tts_models/en/ljspeech/vits--neon\n",
            " 15: tts_models/en/ljspeech/fast_pitch\n",
            " 16: tts_models/en/ljspeech/overflow\n",
            " 17: tts_models/en/ljspeech/neural_hmm\n",
            " 18: tts_models/en/vctk/vits\n",
            " 19: tts_models/en/vctk/fast_pitch\n",
            " 20: tts_models/en/sam/tacotron-DDC\n",
            " 21: tts_models/en/blizzard2013/capacitron-t2-c50\n",
            " 22: tts_models/en/blizzard2013/capacitron-t2-c150_v2\n",
            " 23: tts_models/es/mai/tacotron2-DDC\n",
            " 24: tts_models/es/css10/vits\n",
            " 25: tts_models/fr/mai/tacotron2-DDC\n",
            " 26: tts_models/fr/css10/vits\n",
            " 27: tts_models/uk/mai/glow-tts\n",
            " 28: tts_models/uk/mai/vits\n",
            " 29: tts_models/zh-CN/baker/tacotron2-DDC-GST\n",
            " 30: tts_models/nl/mai/tacotron2-DDC\n",
            " 31: tts_models/nl/css10/vits\n",
            " 32: tts_models/de/thorsten/tacotron2-DCA\n",
            " 33: tts_models/de/thorsten/vits\n",
            " 34: tts_models/de/thorsten/tacotron2-DDC\n",
            " 35: tts_models/de/css10/vits-neon\n",
            " 36: tts_models/ja/kokoro/tacotron2-DDC\n",
            " 37: tts_models/tr/common-voice/glow-tts\n",
            " 38: tts_models/it/mai_female/glow-tts\n",
            " 39: tts_models/it/mai_female/vits\n",
            " 40: tts_models/it/mai_male/glow-tts\n",
            " 41: tts_models/it/mai_male/vits\n",
            " 42: tts_models/ewe/openbible/vits\n",
            " 43: tts_models/hau/openbible/vits\n",
            " 44: tts_models/lin/openbible/vits\n",
            " 45: tts_models/tw_akuapem/openbible/vits\n",
            " 46: tts_models/tw_asante/openbible/vits\n",
            " 47: tts_models/yor/openbible/vits\n",
            " 48: tts_models/hu/css10/vits\n",
            " 49: tts_models/el/cv/vits\n",
            " 50: tts_models/fi/css10/vits\n",
            " 51: tts_models/hr/cv/vits\n",
            " 52: tts_models/lt/cv/vits\n",
            " 53: tts_models/lv/cv/vits\n",
            " 54: tts_models/mt/cv/vits\n",
            " 55: tts_models/pl/mai_female/vits\n",
            " 56: tts_models/pt/cv/vits\n",
            " 57: tts_models/ro/cv/vits\n",
            " 58: tts_models/sk/cv/vits\n",
            " 59: tts_models/sl/cv/vits\n",
            " 60: tts_models/sv/cv/vits\n",
            " 61: tts_models/ca/custom/vits\n",
            " 62: tts_models/fa/custom/glow-tts\n",
            " 63: tts_models/bn/custom/vits-male\n",
            " 64: tts_models/bn/custom/vits-female\n",
            " Name format: type/language/dataset/model\n",
            " 1: vocoder_models/universal/libri-tts/wavegrad\n",
            " 2: vocoder_models/universal/libri-tts/fullband-melgan\n",
            " 3: vocoder_models/en/ek1/wavegrad\n",
            " 4: vocoder_models/en/ljspeech/multiband-melgan\n",
            " 5: vocoder_models/en/ljspeech/hifigan_v2\n",
            " 6: vocoder_models/en/ljspeech/univnet\n",
            " 7: vocoder_models/en/blizzard2013/hifigan_v2\n",
            " 8: vocoder_models/en/vctk/hifigan_v2\n",
            " 9: vocoder_models/en/sam/hifigan_v2\n",
            " 10: vocoder_models/nl/mai/parallel-wavegan\n",
            " 11: vocoder_models/de/thorsten/wavegrad\n",
            " 12: vocoder_models/de/thorsten/fullband-melgan\n",
            " 13: vocoder_models/de/thorsten/hifigan_v1\n",
            " 14: vocoder_models/ja/kokoro/hifigan_v1\n",
            " 15: vocoder_models/uk/mai/multiband-melgan\n",
            " 16: vocoder_models/tr/common-voice/hifigan\n",
            " Name format: type/language/dataset/model\n",
            " 1: voice_conversion_models/multilingual/vctk/freevc24\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!sudo apt-get install espeak-ng\n",
        "!git clone https://github.com/coqui-ai/TTS.git\n",
        "!pip install TTS\n",
        "!tts --list_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX5ftK4TzPUD"
      },
      "source": [
        "4. Download VITS model and Generate Sample Wav File to /content/ljspeech-vits.wav  This will be deleted when your Colab session is closed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy-BadvazVNM"
      },
      "outputs": [],
      "source": [
        "!tts --text \"I am the very model of a modern Major General\" --model_name \"tts_models/en/ljspeech/vits\" --out_path /content/ljspeech-vits.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSthp75hzZvz"
      },
      "source": [
        "Process clips with rnnoise:\n",
        "5. (Optional) Set path for original .wav files\n",
        "Process Audio Clips using rnnoise.py - convert to 22khz mono.  Original author unknown, but thank you for this helpful code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-elDhRpyWbfp"
      },
      "outputs": [],
      "source": [
        "#Location of original wav files. Directory must be named original.\n",
        "orig_wavs=\"/content/drive/MyDrive/original\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. (Optional) Run rnnoise, processed clips will be saved to your Google Drive in a folder named 'converted'"
      ],
      "metadata": {
        "id": "lIoB1kJFVbvm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzfmkv0aWN-7"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import soundfile as sf\n",
        "import pyloudnorm as pyln\n",
        "import sys\n",
        "import glob\n",
        "#src = sys.argv[1]\n",
        "rnn = \"/content/rnnoise/examples/rnnoise_demo\"\n",
        "\n",
        "#paths = Path(src).glob(\"**/*.wav\")\n",
        "#paths = Path(orig_wavs).glob(\"**/*.wav\")\n",
        "paths = glob.glob(os.path.join(orig_wavs, '*.wav'))\n",
        "#print(paths)\n",
        "#paths = Path(orig_wavs).glob(\"*.wav\")\n",
        "\n",
        "for filepath in paths:\n",
        "  target_filepath=Path(str(filepath).replace(\"original\", \"converted\"))\n",
        "  target_dir=os.path.dirname(target_filepath)\n",
        "  #print(target_filepath)\n",
        "  if (str(filepath) == str(target_filepath)):\n",
        "    raise ValueError(\"Source and target path are identical: \" + str(target_filepath))\n",
        "  print(\"From: \" + str(filepath))\n",
        "  print(\"To: \" + str(target_filepath))\n",
        "\t\n",
        "# Stereo to Mono; upsample to 48000Hz\n",
        "# added -G to fix gain, -v 0.95\n",
        "  subprocess.run([\"sox\", \"-G\", \"-v\", \"0.95\", filepath, \"48k.wav\", \"remix\", \"-\", \"rate\", \"48000\"])\n",
        "  subprocess.run([\"sox\", \"48k.wav\", \"-c\", \"1\", \"-r\", \"48000\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"-t\", \"raw\", \"temp.raw\"]) # convert wav to raw\n",
        "  subprocess.run([rnn, \"temp.raw\", \"rnn.raw\"]) # apply rnnoise\n",
        "  subprocess.run([\"sox\", \"-G\", \"-v\", \"0.95\", \"-r\", \"48k\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"rnn.raw\", \"-t\", \"wav\", \"rnn.wav\"]) # convert raw back to wav\n",
        "\n",
        "  subprocess.run([\"mkdir\", \"-p\", str(target_dir)])\n",
        "  subprocess.run([\"sox\", \"rnn.wav\", str(target_filepath), \"remix\", \"-\", \"highpass\", \"100\", \"lowpass\", \"7000\", \"rate\", \"22050\"]) # apply high/low pass filter and change sr to 22050Hz\n",
        "  data, rate = sf.read(target_filepath)\n",
        "\n",
        "# peak normalize audio to -1 dB\n",
        "  peak_normalized_audio = pyln.normalize.peak(data, -1.0)\n",
        "\n",
        "# measure the loudness first\n",
        "  meter = pyln.Meter(rate) # create BS.1770 meter\n",
        "  loudness = meter.integrated_loudness(data)\n",
        "\n",
        "# loudness normalize audio to -25 dB LUFS\n",
        "  loudness_normalized_audio = pyln.normalize.loudness(data, loudness, -25.0)\n",
        "  sf.write(target_filepath, data=loudness_normalized_audio, samplerate=22050)\n",
        "  print(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GqNvXLc9ltKb"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"new-dataset\" #@param {type:\"string\"}\n",
        "output_directory = \"traineroutput\" #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Save the training script with your dataset name, and output dir set. File will be saved in your Google drive dataset folder as train_vits.py"
      ],
      "metadata": {
        "id": "_Hkv4z1YcOfL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyV6G0mozcdk",
        "outputId": "fa48a6ec-fece-4f7a-8e78-06063c6cf313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw------- 1 root root 2946 May  5 10:15 /content/drive/MyDrive/new-dataset/train_vits.py\n"
          ]
        }
      ],
      "source": [
        "code = \"\"\"import os\n",
        "\n",
        "from trainer import Trainer, TrainerArgs\n",
        "\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.models.vits import Vits, VitsAudioConfig\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "\n",
        "#output_path = os.path.dirname(os.path.abspath(__file__))\n",
        "##########################################\n",
        "#Change this to your dataset directory\n",
        "##########################################\n",
        "output_path = \"/content/drive/MyDrive/\"\"\"\n",
        "code = code + dataset_name + \"/\" + output_directory + \"/\" + \"\\\"\"\n",
        "\n",
        "code=code + \"\"\"\n",
        "dataset_config = BaseDatasetConfig(\n",
        "##########################################\n",
        "#Change this to your dataset directory\n",
        "##########################################\n",
        "    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"/content/drive/MyDrive/\"\"\"\n",
        "code = code + dataset_name\n",
        "code=code + \"\"\"\")\n",
        "\n",
        ")\n",
        "audio_config = VitsAudioConfig(\n",
        "    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n",
        ")\n",
        "\n",
        "config = VitsConfig(\n",
        "    audio=audio_config,\n",
        "    run_name=\"vits_ljspeech_ly\",\n",
        "    batch_size=8,\n",
        "    eval_batch_size=8,\n",
        "    batch_group_size=4,\n",
        "#    num_loader_workers=8,\n",
        "    num_loader_workers=4,\n",
        "    num_eval_loader_workers=4,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=100000,\n",
        "    save_step=1000,\n",
        "\tsave_checkpoints=True,\n",
        "\tsave_n_checkpoints=3,\n",
        "\tsave_best_after=1000,\n",
        "    text_cleaner=\"english_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"en-us\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    compute_input_seq_cache=True,\n",
        "    print_step=50,\n",
        "    print_eval=True,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    cudnn_benchmark=False,\n",
        ")\n",
        "\n",
        "# INITIALIZE THE AUDIO PROCESSOR\n",
        "# Audio processor is used for feature extraction and audio I/O.\n",
        "# It mainly serves to the dataloader and the training loggers.\n",
        "ap = AudioProcessor.init_from_config(config)\n",
        "\n",
        "# INITIALIZE THE TOKENIZER\n",
        "# Tokenizer is used to convert text to sequences of token IDs.\n",
        "# config is updated with the default characters if not defined in the config.\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
        "\n",
        "# LOAD DATA SAMPLES\n",
        "# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n",
        "# You can define your custom sample loader returning the list of samples.\n",
        "# Or define your custom formatter and pass it to the `load_tts_samples`.\n",
        "# Check `TTS.tts.datasets.load_tts_samples` for more details.\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")\n",
        "\n",
        "# init model\n",
        "model = Vits(config, ap, tokenizer, speaker_manager=None)\n",
        "\n",
        "# init the trainer and 🚀\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(),\n",
        "    config,\n",
        "    output_path,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples,\n",
        ")\n",
        "trainer.fit()\n",
        "\"\"\"\n",
        "#print(code)\n",
        "myFile = open(\"/content/drive/MyDrive/\" + dataset_name + \"/train_vits.py\", 'w')\n",
        "\n",
        "myFile.write(code)\n",
        "myFile.close()\n",
        "%ls -al /content/drive/MyDrive/$dataset_name/train_vits.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS08OyEVzeJA"
      },
      "source": [
        "8. Fine Tune VITS model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DviNKw7rzkyK"
      },
      "outputs": [],
      "source": [
        "!CUDA_VISIBLE_DEVICES=\"0\" python /content/drive/MyDrive/$dataset_name/train_vits.py --restore_path /root/.local/share/tts/tts_models--en--ljspeech--vits/model_file.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resume a fine tuning session**"
      ],
      "metadata": {
        "id": "C3ydXqTyj-IH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Run the next cell to list all of the saved run direcories.  Copy select the run you want to resume, ctrl+C to copy the name. Paste in the next cell and run it."
      ],
      "metadata": {
        "id": "wQYhcr_we74L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vzxhbrTV8aw",
        "outputId": "524ecc65-725d-49ab-8896-03f3218ec004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "phoneme_cache  vits_ljspeech_ly-May-08-2023_05+58AM-0000000\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/$dataset_name/$output_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0mC-appmrRk"
      },
      "outputs": [],
      "source": [
        "run_name = \"vits_ljspeech_ly-May-08-2023_05+58AM-0000000\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B. Run this cell to list all of the checkpoints in the saved run, then put the checkpoint name in the next cell, and run it."
      ],
      "metadata": {
        "id": "U2YVQGVrfM67"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa2YtWfHZ4Xj",
        "outputId": "3ef76f6a-eefe-46c4-c4e6-d9313049cbe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model_1019667.pth\tconfig.json\n",
            "best_model.pth\t\tevents.out.tfevents.1683525511.bfec041a2818\n",
            "checkpoint_1035000.pth\ttrainer_0_log.txt\n",
            "checkpoint_1036000.pth\ttrain_vits.py\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/$dataset_name/$output_directory/$run_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z59OjTpZ4ha"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"checkpoint_1036000.pth\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. Run the next cell to restore a previous fine tuning session using your dataset name, trainer output directory, and model checkpoint set"
      ],
      "metadata": {
        "id": "f2WQUoLhfrUu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiM4XGXgnnIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0e73b84-178f-468a-b8a9-675f988f8d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:12:40) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 47/49 -- GLOBAL_STEP: 1050650\u001b[0m\n",
            "     | > loss_disc: 2.49486  (2.46483)\n",
            "     | > loss_disc_real_0: 0.10324  (0.08988)\n",
            "     | > loss_disc_real_1: 0.19468  (0.21956)\n",
            "     | > loss_disc_real_2: 0.22690  (0.22676)\n",
            "     | > loss_disc_real_3: 0.25009  (0.23619)\n",
            "     | > loss_disc_real_4: 0.28320  (0.24414)\n",
            "     | > loss_disc_real_5: 0.23093  (0.23217)\n",
            "     | > loss_0: 2.49486  (2.46483)\n",
            "     | > grad_norm_0: 6.55365  (7.61557)\n",
            "     | > loss_gen: 2.23336  (2.31089)\n",
            "     | > loss_kl: 1.71291  (1.24175)\n",
            "     | > loss_feat: 4.75750  (4.91091)\n",
            "     | > loss_mel: 15.38094  (14.61609)\n",
            "     | > loss_duration: 1.17238  (1.04582)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 25.25708  (24.12545)\n",
            "     | > grad_norm_1: 247.33228  (328.91617)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.70070  (0.59570)\n",
            "     | > loader_time: 0.00560  (0.00677)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.87928  (2.87928)\n",
            "     | > loss_disc_real_0: 0.38598  (0.38598)\n",
            "     | > loss_disc_real_1: 0.27383  (0.27383)\n",
            "     | > loss_disc_real_2: 0.19378  (0.19378)\n",
            "     | > loss_disc_real_3: 0.21028  (0.21028)\n",
            "     | > loss_disc_real_4: 0.21555  (0.21555)\n",
            "     | > loss_disc_real_5: 0.20201  (0.20201)\n",
            "     | > loss_0: 2.87928  (2.87928)\n",
            "     | > loss_gen: 2.00524  (2.00524)\n",
            "     | > loss_kl: 7.51270  (7.51270)\n",
            "     | > loss_feat: 4.25958  (4.25958)\n",
            "     | > loss_mel: 15.39657  (15.39657)\n",
            "     | > loss_duration: 2.39111  (2.39111)\n",
            "     | > loss_1: 31.56519  (31.56519)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.56629 \u001b[0m(+0.19364)\n",
            "     | > avg_loss_disc:\u001b[92m 2.87928 \u001b[0m(-0.21655)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.38598 \u001b[0m(-0.14818)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.27383 \u001b[0m(+0.00414)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.19378 \u001b[0m(+0.00628)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.21028 \u001b[0m(+0.05949)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.21555 \u001b[0m(-0.01562)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.20201 \u001b[0m(+0.08766)\n",
            "     | > avg_loss_0:\u001b[92m 2.87928 \u001b[0m(-0.21655)\n",
            "     | > avg_loss_gen:\u001b[91m 2.00524 \u001b[0m(+0.13454)\n",
            "     | > avg_loss_kl:\u001b[92m 7.51270 \u001b[0m(-0.24878)\n",
            "     | > avg_loss_feat:\u001b[92m 4.25958 \u001b[0m(-0.62036)\n",
            "     | > avg_loss_mel:\u001b[92m 15.39657 \u001b[0m(-1.21212)\n",
            "     | > avg_loss_duration:\u001b[91m 2.39111 \u001b[0m(+0.00998)\n",
            "     | > avg_loss_1:\u001b[92m 31.56519 \u001b[0m(-1.93673)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 299/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:13:18) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 48/49 -- GLOBAL_STEP: 1050700\u001b[0m\n",
            "     | > loss_disc: 2.50720  (2.44826)\n",
            "     | > loss_disc_real_0: 0.08319  (0.07589)\n",
            "     | > loss_disc_real_1: 0.20602  (0.22045)\n",
            "     | > loss_disc_real_2: 0.19570  (0.22695)\n",
            "     | > loss_disc_real_3: 0.24000  (0.23445)\n",
            "     | > loss_disc_real_4: 0.22722  (0.24097)\n",
            "     | > loss_disc_real_5: 0.22681  (0.23759)\n",
            "     | > loss_0: 2.50720  (2.44826)\n",
            "     | > grad_norm_0: 3.82074  (5.91445)\n",
            "     | > loss_gen: 2.13531  (2.31876)\n",
            "     | > loss_kl: 1.81375  (1.22241)\n",
            "     | > loss_feat: 4.88521  (4.99791)\n",
            "     | > loss_mel: 15.15158  (14.65165)\n",
            "     | > loss_duration: 1.16343  (1.04562)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 25.14928  (24.23635)\n",
            "     | > grad_norm_1: 140.50185  (176.16739)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.74960  (0.60389)\n",
            "     | > loader_time: 0.00610  (0.00726)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.90996  (2.90996)\n",
            "     | > loss_disc_real_0: 0.33950  (0.33950)\n",
            "     | > loss_disc_real_1: 0.12223  (0.12223)\n",
            "     | > loss_disc_real_2: 0.17045  (0.17045)\n",
            "     | > loss_disc_real_3: 0.20122  (0.20122)\n",
            "     | > loss_disc_real_4: 0.24073  (0.24073)\n",
            "     | > loss_disc_real_5: 0.28902  (0.28902)\n",
            "     | > loss_0: 2.90996  (2.90996)\n",
            "     | > loss_gen: 1.88607  (1.88607)\n",
            "     | > loss_kl: 7.65024  (7.65024)\n",
            "     | > loss_feat: 4.05025  (4.05025)\n",
            "     | > loss_mel: 16.54318  (16.54318)\n",
            "     | > loss_duration: 2.36620  (2.36620)\n",
            "     | > loss_1: 32.49594  (32.49594)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.35757 \u001b[0m(-0.20871)\n",
            "     | > avg_loss_disc:\u001b[91m 2.90996 \u001b[0m(+0.03067)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.33950 \u001b[0m(-0.04648)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.12223 \u001b[0m(-0.15160)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.17045 \u001b[0m(-0.02333)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.20122 \u001b[0m(-0.00906)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.24073 \u001b[0m(+0.02518)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.28902 \u001b[0m(+0.08701)\n",
            "     | > avg_loss_0:\u001b[91m 2.90996 \u001b[0m(+0.03067)\n",
            "     | > avg_loss_gen:\u001b[92m 1.88607 \u001b[0m(-0.11917)\n",
            "     | > avg_loss_kl:\u001b[91m 7.65024 \u001b[0m(+0.13754)\n",
            "     | > avg_loss_feat:\u001b[92m 4.05025 \u001b[0m(-0.20933)\n",
            "     | > avg_loss_mel:\u001b[91m 16.54318 \u001b[0m(+1.14662)\n",
            "     | > avg_loss_duration:\u001b[92m 2.36620 \u001b[0m(-0.02491)\n",
            "     | > avg_loss_1:\u001b[91m 32.49594 \u001b[0m(+0.93075)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 300/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:13:54) \u001b[0m\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 3.05367  (3.05367)\n",
            "     | > loss_disc_real_0: 0.53267  (0.53267)\n",
            "     | > loss_disc_real_1: 0.16311  (0.16311)\n",
            "     | > loss_disc_real_2: 0.24645  (0.24645)\n",
            "     | > loss_disc_real_3: 0.23476  (0.23476)\n",
            "     | > loss_disc_real_4: 0.26521  (0.26521)\n",
            "     | > loss_disc_real_5: 0.19540  (0.19540)\n",
            "     | > loss_0: 3.05367  (3.05367)\n",
            "     | > loss_gen: 2.07862  (2.07862)\n",
            "     | > loss_kl: 7.31774  (7.31774)\n",
            "     | > loss_feat: 3.75094  (3.75094)\n",
            "     | > loss_mel: 16.64366  (16.64366)\n",
            "     | > loss_duration: 2.23748  (2.23748)\n",
            "     | > loss_1: 32.02844  (32.02844)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.49618 \u001b[0m(+0.13860)\n",
            "     | > avg_loss_disc:\u001b[91m 3.05367 \u001b[0m(+0.14372)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.53267 \u001b[0m(+0.19317)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.16311 \u001b[0m(+0.04088)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.24645 \u001b[0m(+0.07600)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.23476 \u001b[0m(+0.03354)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.26521 \u001b[0m(+0.02448)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.19540 \u001b[0m(-0.09362)\n",
            "     | > avg_loss_0:\u001b[91m 3.05367 \u001b[0m(+0.14372)\n",
            "     | > avg_loss_gen:\u001b[91m 2.07862 \u001b[0m(+0.19255)\n",
            "     | > avg_loss_kl:\u001b[92m 7.31774 \u001b[0m(-0.33249)\n",
            "     | > avg_loss_feat:\u001b[92m 3.75094 \u001b[0m(-0.29931)\n",
            "     | > avg_loss_mel:\u001b[91m 16.64366 \u001b[0m(+0.10048)\n",
            "     | > avg_loss_duration:\u001b[92m 2.23748 \u001b[0m(-0.12871)\n",
            "     | > avg_loss_1:\u001b[92m 32.02844 \u001b[0m(-0.46750)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 301/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:14:32) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0/49 -- GLOBAL_STEP: 1050750\u001b[0m\n",
            "     | > loss_disc: 2.46946  (2.46946)\n",
            "     | > loss_disc_real_0: 0.06476  (0.06476)\n",
            "     | > loss_disc_real_1: 0.14391  (0.14391)\n",
            "     | > loss_disc_real_2: 0.21205  (0.21205)\n",
            "     | > loss_disc_real_3: 0.22905  (0.22905)\n",
            "     | > loss_disc_real_4: 0.22388  (0.22388)\n",
            "     | > loss_disc_real_5: 0.22164  (0.22164)\n",
            "     | > loss_0: 2.46946  (2.46946)\n",
            "     | > grad_norm_0: 12.24626  (12.24626)\n",
            "     | > loss_gen: 2.28137  (2.28137)\n",
            "     | > loss_kl: 1.33861  (1.33861)\n",
            "     | > loss_feat: 4.92821  (4.92821)\n",
            "     | > loss_mel: 13.49749  (13.49749)\n",
            "     | > loss_duration: 1.05409  (1.05409)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.09977  (23.09977)\n",
            "     | > grad_norm_1: 334.17761  (334.17761)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.72820  (0.72820)\n",
            "     | > loader_time: 0.41220  (0.41222)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.86378  (2.86378)\n",
            "     | > loss_disc_real_0: 0.39420  (0.39420)\n",
            "     | > loss_disc_real_1: 0.27374  (0.27374)\n",
            "     | > loss_disc_real_2: 0.20664  (0.20664)\n",
            "     | > loss_disc_real_3: 0.20767  (0.20767)\n",
            "     | > loss_disc_real_4: 0.24161  (0.24161)\n",
            "     | > loss_disc_real_5: 0.23134  (0.23134)\n",
            "     | > loss_0: 2.86378  (2.86378)\n",
            "     | > loss_gen: 2.22076  (2.22076)\n",
            "     | > loss_kl: 7.18276  (7.18276)\n",
            "     | > loss_feat: 4.95477  (4.95477)\n",
            "     | > loss_mel: 16.55837  (16.55837)\n",
            "     | > loss_duration: 2.42861  (2.42861)\n",
            "     | > loss_1: 33.34528  (33.34528)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.36047 \u001b[0m(-0.13571)\n",
            "     | > avg_loss_disc:\u001b[92m 2.86378 \u001b[0m(-0.18989)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.39420 \u001b[0m(-0.13847)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.27374 \u001b[0m(+0.11063)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.20664 \u001b[0m(-0.03981)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.20767 \u001b[0m(-0.02709)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.24161 \u001b[0m(-0.02360)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.23134 \u001b[0m(+0.03594)\n",
            "     | > avg_loss_0:\u001b[92m 2.86378 \u001b[0m(-0.18989)\n",
            "     | > avg_loss_gen:\u001b[91m 2.22076 \u001b[0m(+0.14214)\n",
            "     | > avg_loss_kl:\u001b[92m 7.18276 \u001b[0m(-0.13499)\n",
            "     | > avg_loss_feat:\u001b[91m 4.95477 \u001b[0m(+1.20384)\n",
            "     | > avg_loss_mel:\u001b[92m 16.55837 \u001b[0m(-0.08528)\n",
            "     | > avg_loss_duration:\u001b[91m 2.42861 \u001b[0m(+0.19113)\n",
            "     | > avg_loss_1:\u001b[91m 33.34528 \u001b[0m(+1.31683)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 302/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:15:08) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 1/49 -- GLOBAL_STEP: 1050800\u001b[0m\n",
            "     | > loss_disc: 2.37346  (2.37346)\n",
            "     | > loss_disc_real_0: 0.03405  (0.03405)\n",
            "     | > loss_disc_real_1: 0.28489  (0.28489)\n",
            "     | > loss_disc_real_2: 0.22984  (0.22984)\n",
            "     | > loss_disc_real_3: 0.22078  (0.22078)\n",
            "     | > loss_disc_real_4: 0.25927  (0.25927)\n",
            "     | > loss_disc_real_5: 0.22425  (0.22425)\n",
            "     | > loss_0: 2.37346  (2.37346)\n",
            "     | > grad_norm_0: 13.26413  (13.26413)\n",
            "     | > loss_gen: 2.63829  (2.63829)\n",
            "     | > loss_kl: 0.98515  (0.98515)\n",
            "     | > loss_feat: 5.46704  (5.46704)\n",
            "     | > loss_mel: 13.44728  (13.44728)\n",
            "     | > loss_duration: 0.93511  (0.93511)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.47285  (23.47285)\n",
            "     | > grad_norm_1: 612.83154  (612.83154)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.49760  (0.49765)\n",
            "     | > loader_time: 0.00400  (0.00396)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.76745  (2.76745)\n",
            "     | > loss_disc_real_0: 0.23688  (0.23688)\n",
            "     | > loss_disc_real_1: 0.23416  (0.23416)\n",
            "     | > loss_disc_real_2: 0.19382  (0.19382)\n",
            "     | > loss_disc_real_3: 0.20781  (0.20781)\n",
            "     | > loss_disc_real_4: 0.22511  (0.22511)\n",
            "     | > loss_disc_real_5: 0.25894  (0.25894)\n",
            "     | > loss_0: 2.76745  (2.76745)\n",
            "     | > loss_gen: 1.82916  (1.82916)\n",
            "     | > loss_kl: 7.44865  (7.44865)\n",
            "     | > loss_feat: 3.95074  (3.95074)\n",
            "     | > loss_mel: 16.12060  (16.12060)\n",
            "     | > loss_duration: 2.35404  (2.35404)\n",
            "     | > loss_1: 31.70318  (31.70318)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.36003 \u001b[0m(-0.00044)\n",
            "     | > avg_loss_disc:\u001b[92m 2.76745 \u001b[0m(-0.09633)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.23688 \u001b[0m(-0.15731)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.23416 \u001b[0m(-0.03959)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.19382 \u001b[0m(-0.01281)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.20781 \u001b[0m(+0.00014)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.22511 \u001b[0m(-0.01650)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.25894 \u001b[0m(+0.02759)\n",
            "     | > avg_loss_0:\u001b[92m 2.76745 \u001b[0m(-0.09633)\n",
            "     | > avg_loss_gen:\u001b[92m 1.82916 \u001b[0m(-0.39160)\n",
            "     | > avg_loss_kl:\u001b[91m 7.44865 \u001b[0m(+0.26589)\n",
            "     | > avg_loss_feat:\u001b[92m 3.95074 \u001b[0m(-1.00403)\n",
            "     | > avg_loss_mel:\u001b[92m 16.12060 \u001b[0m(-0.43777)\n",
            "     | > avg_loss_duration:\u001b[92m 2.35404 \u001b[0m(-0.07457)\n",
            "     | > avg_loss_1:\u001b[92m 31.70318 \u001b[0m(-1.64209)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 303/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:15:45) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 2/49 -- GLOBAL_STEP: 1050850\u001b[0m\n",
            "     | > loss_disc: 2.59593  (2.44436)\n",
            "     | > loss_disc_real_0: 0.13165  (0.08344)\n",
            "     | > loss_disc_real_1: 0.23849  (0.23430)\n",
            "     | > loss_disc_real_2: 0.23373  (0.21317)\n",
            "     | > loss_disc_real_3: 0.30123  (0.26835)\n",
            "     | > loss_disc_real_4: 0.23947  (0.22828)\n",
            "     | > loss_disc_real_5: 0.26212  (0.20857)\n",
            "     | > loss_0: 2.59593  (2.44436)\n",
            "     | > grad_norm_0: 9.92764  (9.37821)\n",
            "     | > loss_gen: 2.77737  (2.78704)\n",
            "     | > loss_kl: 0.39588  (0.93791)\n",
            "     | > loss_feat: 4.72173  (5.36694)\n",
            "     | > loss_mel: 13.35818  (13.23574)\n",
            "     | > loss_duration: 0.97526  (1.00918)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 22.22842  (23.33682)\n",
            "     | > grad_norm_1: 69.51904  (207.82120)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.92730  (0.70645)\n",
            "     | > loader_time: 0.00760  (0.00693)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.81926  (2.81926)\n",
            "     | > loss_disc_real_0: 0.50145  (0.50145)\n",
            "     | > loss_disc_real_1: 0.24297  (0.24297)\n",
            "     | > loss_disc_real_2: 0.20840  (0.20840)\n",
            "     | > loss_disc_real_3: 0.25631  (0.25631)\n",
            "     | > loss_disc_real_4: 0.25659  (0.25659)\n",
            "     | > loss_disc_real_5: 0.25284  (0.25284)\n",
            "     | > loss_0: 2.81926  (2.81926)\n",
            "     | > loss_gen: 2.48164  (2.48164)\n",
            "     | > loss_kl: 6.92380  (6.92380)\n",
            "     | > loss_feat: 4.19992  (4.19992)\n",
            "     | > loss_mel: 16.26128  (16.26128)\n",
            "     | > loss_duration: 2.28680  (2.28680)\n",
            "     | > loss_1: 32.15343  (32.15343)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.35844 \u001b[0m(-0.00159)\n",
            "     | > avg_loss_disc:\u001b[91m 2.81926 \u001b[0m(+0.05181)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.50145 \u001b[0m(+0.26456)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.24297 \u001b[0m(+0.00882)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.20840 \u001b[0m(+0.01458)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.25631 \u001b[0m(+0.04850)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.25659 \u001b[0m(+0.03148)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.25284 \u001b[0m(-0.00610)\n",
            "     | > avg_loss_0:\u001b[91m 2.81926 \u001b[0m(+0.05181)\n",
            "     | > avg_loss_gen:\u001b[91m 2.48164 \u001b[0m(+0.65248)\n",
            "     | > avg_loss_kl:\u001b[92m 6.92380 \u001b[0m(-0.52485)\n",
            "     | > avg_loss_feat:\u001b[91m 4.19992 \u001b[0m(+0.24918)\n",
            "     | > avg_loss_mel:\u001b[91m 16.26128 \u001b[0m(+0.14068)\n",
            "     | > avg_loss_duration:\u001b[92m 2.28680 \u001b[0m(-0.06724)\n",
            "     | > avg_loss_1:\u001b[91m 32.15343 \u001b[0m(+0.45025)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 304/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:16:22) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 3/49 -- GLOBAL_STEP: 1050900\u001b[0m\n",
            "     | > loss_disc: 2.50024  (2.44875)\n",
            "     | > loss_disc_real_0: 0.06056  (0.07906)\n",
            "     | > loss_disc_real_1: 0.19206  (0.23503)\n",
            "     | > loss_disc_real_2: 0.31209  (0.26000)\n",
            "     | > loss_disc_real_3: 0.24913  (0.23542)\n",
            "     | > loss_disc_real_4: 0.24473  (0.25396)\n",
            "     | > loss_disc_real_5: 0.31483  (0.26755)\n",
            "     | > loss_0: 2.50024  (2.44875)\n",
            "     | > grad_norm_0: 18.99765  (13.61668)\n",
            "     | > loss_gen: 2.49811  (2.68673)\n",
            "     | > loss_kl: 0.55564  (0.78471)\n",
            "     | > loss_feat: 5.06299  (5.03319)\n",
            "     | > loss_mel: 13.99521  (13.59050)\n",
            "     | > loss_duration: 1.08091  (0.98703)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.19286  (23.08215)\n",
            "     | > grad_norm_1: 660.63336  (597.07275)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.56440  (0.51958)\n",
            "     | > loader_time: 0.00350  (0.00620)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.88323  (2.88323)\n",
            "     | > loss_disc_real_0: 0.46012  (0.46012)\n",
            "     | > loss_disc_real_1: 0.25439  (0.25439)\n",
            "     | > loss_disc_real_2: 0.24243  (0.24243)\n",
            "     | > loss_disc_real_3: 0.22488  (0.22488)\n",
            "     | > loss_disc_real_4: 0.22992  (0.22992)\n",
            "     | > loss_disc_real_5: 0.20829  (0.20829)\n",
            "     | > loss_0: 2.88323  (2.88323)\n",
            "     | > loss_gen: 2.16350  (2.16350)\n",
            "     | > loss_kl: 7.08130  (7.08130)\n",
            "     | > loss_feat: 4.27998  (4.27998)\n",
            "     | > loss_mel: 16.59374  (16.59374)\n",
            "     | > loss_duration: 2.41062  (2.41062)\n",
            "     | > loss_1: 32.52914  (32.52914)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.35567 \u001b[0m(-0.00277)\n",
            "     | > avg_loss_disc:\u001b[91m 2.88323 \u001b[0m(+0.06398)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.46012 \u001b[0m(-0.04133)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.25439 \u001b[0m(+0.01141)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.24243 \u001b[0m(+0.03403)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.22488 \u001b[0m(-0.03143)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.22992 \u001b[0m(-0.02666)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.20829 \u001b[0m(-0.04455)\n",
            "     | > avg_loss_0:\u001b[91m 2.88323 \u001b[0m(+0.06398)\n",
            "     | > avg_loss_gen:\u001b[92m 2.16350 \u001b[0m(-0.31814)\n",
            "     | > avg_loss_kl:\u001b[91m 7.08130 \u001b[0m(+0.15750)\n",
            "     | > avg_loss_feat:\u001b[91m 4.27998 \u001b[0m(+0.08005)\n",
            "     | > avg_loss_mel:\u001b[91m 16.59374 \u001b[0m(+0.33246)\n",
            "     | > avg_loss_duration:\u001b[91m 2.41062 \u001b[0m(+0.12383)\n",
            "     | > avg_loss_1:\u001b[91m 32.52914 \u001b[0m(+0.37571)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 305/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:16:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 4/49 -- GLOBAL_STEP: 1050950\u001b[0m\n",
            "     | > loss_disc: 2.27390  (2.33809)\n",
            "     | > loss_disc_real_0: 0.10138  (0.08302)\n",
            "     | > loss_disc_real_1: 0.17403  (0.22093)\n",
            "     | > loss_disc_real_2: 0.20513  (0.22055)\n",
            "     | > loss_disc_real_3: 0.21242  (0.21136)\n",
            "     | > loss_disc_real_4: 0.18942  (0.22533)\n",
            "     | > loss_disc_real_5: 0.25442  (0.23384)\n",
            "     | > loss_0: 2.27390  (2.33809)\n",
            "     | > grad_norm_0: 7.66220  (6.68641)\n",
            "     | > loss_gen: 2.46582  (2.64235)\n",
            "     | > loss_kl: 0.36345  (0.83231)\n",
            "     | > loss_feat: 5.76339  (5.43811)\n",
            "     | > loss_mel: 13.81371  (13.43788)\n",
            "     | > loss_duration: 1.02773  (1.00545)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.43409  (23.35610)\n",
            "     | > grad_norm_1: 652.72314  (337.08179)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.48140  (0.50394)\n",
            "     | > loader_time: 0.01150  (0.00816)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.94823  (2.94823)\n",
            "     | > loss_disc_real_0: 0.55844  (0.55844)\n",
            "     | > loss_disc_real_1: 0.20543  (0.20543)\n",
            "     | > loss_disc_real_2: 0.17145  (0.17145)\n",
            "     | > loss_disc_real_3: 0.17822  (0.17822)\n",
            "     | > loss_disc_real_4: 0.19838  (0.19838)\n",
            "     | > loss_disc_real_5: 0.16407  (0.16407)\n",
            "     | > loss_0: 2.94823  (2.94823)\n",
            "     | > loss_gen: 1.97437  (1.97437)\n",
            "     | > loss_kl: 7.36150  (7.36150)\n",
            "     | > loss_feat: 4.63303  (4.63303)\n",
            "     | > loss_mel: 15.58852  (15.58852)\n",
            "     | > loss_duration: 2.31600  (2.31600)\n",
            "     | > loss_1: 31.87342  (31.87342)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.34365 \u001b[0m(-0.01202)\n",
            "     | > avg_loss_disc:\u001b[91m 2.94823 \u001b[0m(+0.06500)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.55844 \u001b[0m(+0.09833)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.20543 \u001b[0m(-0.04896)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.17145 \u001b[0m(-0.07098)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.17822 \u001b[0m(-0.04666)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.19838 \u001b[0m(-0.03154)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.16407 \u001b[0m(-0.04421)\n",
            "     | > avg_loss_0:\u001b[91m 2.94823 \u001b[0m(+0.06500)\n",
            "     | > avg_loss_gen:\u001b[92m 1.97437 \u001b[0m(-0.18913)\n",
            "     | > avg_loss_kl:\u001b[91m 7.36150 \u001b[0m(+0.28020)\n",
            "     | > avg_loss_feat:\u001b[91m 4.63303 \u001b[0m(+0.35305)\n",
            "     | > avg_loss_mel:\u001b[92m 15.58852 \u001b[0m(-1.00523)\n",
            "     | > avg_loss_duration:\u001b[92m 2.31600 \u001b[0m(-0.09462)\n",
            "     | > avg_loss_1:\u001b[92m 31.87342 \u001b[0m(-0.65573)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 306/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:17:35) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 5/49 -- GLOBAL_STEP: 1051000\u001b[0m\n",
            "     | > loss_disc: 2.47300  (2.39763)\n",
            "     | > loss_disc_real_0: 0.04369  (0.07533)\n",
            "     | > loss_disc_real_1: 0.24510  (0.21816)\n",
            "     | > loss_disc_real_2: 0.23763  (0.23165)\n",
            "     | > loss_disc_real_3: 0.25342  (0.22770)\n",
            "     | > loss_disc_real_4: 0.24929  (0.25033)\n",
            "     | > loss_disc_real_5: 0.25908  (0.23600)\n",
            "     | > loss_0: 2.47300  (2.39763)\n",
            "     | > grad_norm_0: 3.81309  (6.49956)\n",
            "     | > loss_gen: 2.30282  (2.53672)\n",
            "     | > loss_kl: 1.15701  (1.11377)\n",
            "     | > loss_feat: 5.33352  (5.09312)\n",
            "     | > loss_mel: 13.72193  (13.43345)\n",
            "     | > loss_duration: 0.98216  (1.02122)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.49744  (23.19828)\n",
            "     | > grad_norm_1: 93.44987  (139.39021)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.56850  (0.51941)\n",
            "     | > loader_time: 0.00990  (0.00664)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000/checkpoint_1051000.pth\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.98773  (2.98773)\n",
            "     | > loss_disc_real_0: 0.55986  (0.55986)\n",
            "     | > loss_disc_real_1: 0.20192  (0.20192)\n",
            "     | > loss_disc_real_2: 0.20085  (0.20085)\n",
            "     | > loss_disc_real_3: 0.24085  (0.24085)\n",
            "     | > loss_disc_real_4: 0.24695  (0.24695)\n",
            "     | > loss_disc_real_5: 0.19932  (0.19932)\n",
            "     | > loss_0: 2.98773  (2.98773)\n",
            "     | > loss_gen: 2.14255  (2.14255)\n",
            "     | > loss_kl: 7.43338  (7.43338)\n",
            "     | > loss_feat: 4.60958  (4.60958)\n",
            "     | > loss_mel: 15.92081  (15.92081)\n",
            "     | > loss_duration: 2.34239  (2.34239)\n",
            "     | > loss_1: 32.44870  (32.44870)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.36864 \u001b[0m(+0.02499)\n",
            "     | > avg_loss_disc:\u001b[91m 2.98773 \u001b[0m(+0.03949)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.55986 \u001b[0m(+0.00141)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.20192 \u001b[0m(-0.00351)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.20085 \u001b[0m(+0.02940)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.24085 \u001b[0m(+0.06264)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.24695 \u001b[0m(+0.04857)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.19932 \u001b[0m(+0.03525)\n",
            "     | > avg_loss_0:\u001b[91m 2.98773 \u001b[0m(+0.03949)\n",
            "     | > avg_loss_gen:\u001b[91m 2.14255 \u001b[0m(+0.16818)\n",
            "     | > avg_loss_kl:\u001b[91m 7.43338 \u001b[0m(+0.07188)\n",
            "     | > avg_loss_feat:\u001b[92m 4.60958 \u001b[0m(-0.02345)\n",
            "     | > avg_loss_mel:\u001b[91m 15.92081 \u001b[0m(+0.33229)\n",
            "     | > avg_loss_duration:\u001b[91m 2.34239 \u001b[0m(+0.02638)\n",
            "     | > avg_loss_1:\u001b[91m 32.44870 \u001b[0m(+0.57529)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 307/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:18:21) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 6/49 -- GLOBAL_STEP: 1051050\u001b[0m\n",
            "     | > loss_disc: 2.44478  (2.45643)\n",
            "     | > loss_disc_real_0: 0.04545  (0.10328)\n",
            "     | > loss_disc_real_1: 0.16074  (0.21106)\n",
            "     | > loss_disc_real_2: 0.21507  (0.21859)\n",
            "     | > loss_disc_real_3: 0.30236  (0.25466)\n",
            "     | > loss_disc_real_4: 0.27121  (0.24584)\n",
            "     | > loss_disc_real_5: 0.30131  (0.26250)\n",
            "     | > loss_0: 2.44478  (2.45643)\n",
            "     | > grad_norm_0: 11.11027  (9.22959)\n",
            "     | > loss_gen: 2.29125  (2.51345)\n",
            "     | > loss_kl: 0.18228  (0.68063)\n",
            "     | > loss_feat: 5.11194  (5.26571)\n",
            "     | > loss_mel: 13.40460  (13.69495)\n",
            "     | > loss_duration: 0.90826  (0.95905)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 21.89833  (23.11379)\n",
            "     | > grad_norm_1: 275.72339  (199.37297)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.61520  (0.53514)\n",
            "     | > loader_time: 0.00330  (0.00718)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.69437  (2.69437)\n",
            "     | > loss_disc_real_0: 0.52447  (0.52447)\n",
            "     | > loss_disc_real_1: 0.27427  (0.27427)\n",
            "     | > loss_disc_real_2: 0.18182  (0.18182)\n",
            "     | > loss_disc_real_3: 0.20948  (0.20948)\n",
            "     | > loss_disc_real_4: 0.24017  (0.24017)\n",
            "     | > loss_disc_real_5: 0.24899  (0.24899)\n",
            "     | > loss_0: 2.69437  (2.69437)\n",
            "     | > loss_gen: 2.49222  (2.49222)\n",
            "     | > loss_kl: 7.10658  (7.10658)\n",
            "     | > loss_feat: 4.63978  (4.63978)\n",
            "     | > loss_mel: 15.45290  (15.45290)\n",
            "     | > loss_duration: 2.32070  (2.32070)\n",
            "     | > loss_1: 32.01218  (32.01218)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.35474 \u001b[0m(-0.01390)\n",
            "     | > avg_loss_disc:\u001b[92m 2.69437 \u001b[0m(-0.29336)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.52447 \u001b[0m(-0.03539)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.27427 \u001b[0m(+0.07235)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.18182 \u001b[0m(-0.01903)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.20948 \u001b[0m(-0.03137)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.24017 \u001b[0m(-0.00677)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.24899 \u001b[0m(+0.04967)\n",
            "     | > avg_loss_0:\u001b[92m 2.69437 \u001b[0m(-0.29336)\n",
            "     | > avg_loss_gen:\u001b[91m 2.49222 \u001b[0m(+0.34967)\n",
            "     | > avg_loss_kl:\u001b[92m 7.10658 \u001b[0m(-0.32680)\n",
            "     | > avg_loss_feat:\u001b[91m 4.63978 \u001b[0m(+0.03021)\n",
            "     | > avg_loss_mel:\u001b[92m 15.45290 \u001b[0m(-0.46791)\n",
            "     | > avg_loss_duration:\u001b[92m 2.32070 \u001b[0m(-0.02168)\n",
            "     | > avg_loss_1:\u001b[92m 32.01218 \u001b[0m(-0.43652)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 308/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:18:58) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 7/49 -- GLOBAL_STEP: 1051100\u001b[0m\n",
            "     | > loss_disc: 2.24584  (2.38655)\n",
            "     | > loss_disc_real_0: 0.03145  (0.07488)\n",
            "     | > loss_disc_real_1: 0.14524  (0.19951)\n",
            "     | > loss_disc_real_2: 0.18229  (0.21724)\n",
            "     | > loss_disc_real_3: 0.16551  (0.23995)\n",
            "     | > loss_disc_real_4: 0.21387  (0.24313)\n",
            "     | > loss_disc_real_5: 0.17762  (0.24674)\n",
            "     | > loss_0: 2.24584  (2.38655)\n",
            "     | > grad_norm_0: 7.94659  (7.98385)\n",
            "     | > loss_gen: 2.47319  (2.50973)\n",
            "     | > loss_kl: 1.90952  (1.13251)\n",
            "     | > loss_feat: 6.06479  (5.48852)\n",
            "     | > loss_mel: 14.27319  (13.92455)\n",
            "     | > loss_duration: 0.94184  (0.99417)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 25.66253  (24.04948)\n",
            "     | > grad_norm_1: 660.58691  (532.23975)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.48280  (0.50035)\n",
            "     | > loader_time: 0.00450  (0.00732)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.86364  (2.86364)\n",
            "     | > loss_disc_real_0: 0.50261  (0.50261)\n",
            "     | > loss_disc_real_1: 0.17918  (0.17918)\n",
            "     | > loss_disc_real_2: 0.21882  (0.21882)\n",
            "     | > loss_disc_real_3: 0.21860  (0.21860)\n",
            "     | > loss_disc_real_4: 0.17884  (0.17884)\n",
            "     | > loss_disc_real_5: 0.18159  (0.18159)\n",
            "     | > loss_0: 2.86364  (2.86364)\n",
            "     | > loss_gen: 1.87399  (1.87399)\n",
            "     | > loss_kl: 7.66600  (7.66600)\n",
            "     | > loss_feat: 4.34255  (4.34255)\n",
            "     | > loss_mel: 15.54549  (15.54549)\n",
            "     | > loss_duration: 2.37236  (2.37236)\n",
            "     | > loss_1: 31.80039  (31.80039)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.34268 \u001b[0m(-0.01205)\n",
            "     | > avg_loss_disc:\u001b[91m 2.86364 \u001b[0m(+0.16927)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.50261 \u001b[0m(-0.02185)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.17918 \u001b[0m(-0.09509)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.21882 \u001b[0m(+0.03701)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.21860 \u001b[0m(+0.00911)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.17884 \u001b[0m(-0.06134)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.18159 \u001b[0m(-0.06741)\n",
            "     | > avg_loss_0:\u001b[91m 2.86364 \u001b[0m(+0.16927)\n",
            "     | > avg_loss_gen:\u001b[92m 1.87399 \u001b[0m(-0.61823)\n",
            "     | > avg_loss_kl:\u001b[91m 7.66600 \u001b[0m(+0.55942)\n",
            "     | > avg_loss_feat:\u001b[92m 4.34255 \u001b[0m(-0.29723)\n",
            "     | > avg_loss_mel:\u001b[91m 15.54549 \u001b[0m(+0.09259)\n",
            "     | > avg_loss_duration:\u001b[91m 2.37236 \u001b[0m(+0.05166)\n",
            "     | > avg_loss_1:\u001b[92m 31.80039 \u001b[0m(-0.21179)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 309/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:19:34) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 8/49 -- GLOBAL_STEP: 1051150\u001b[0m\n",
            "     | > loss_disc: 2.39023  (2.37523)\n",
            "     | > loss_disc_real_0: 0.07859  (0.07433)\n",
            "     | > loss_disc_real_1: 0.21920  (0.20863)\n",
            "     | > loss_disc_real_2: 0.20892  (0.22224)\n",
            "     | > loss_disc_real_3: 0.27824  (0.22788)\n",
            "     | > loss_disc_real_4: 0.23859  (0.23577)\n",
            "     | > loss_disc_real_5: 0.23192  (0.24118)\n",
            "     | > loss_0: 2.39023  (2.37523)\n",
            "     | > grad_norm_0: 13.25327  (9.00624)\n",
            "     | > loss_gen: 2.49308  (2.52566)\n",
            "     | > loss_kl: 1.00323  (0.97958)\n",
            "     | > loss_feat: 5.53904  (5.44601)\n",
            "     | > loss_mel: 14.05060  (13.67625)\n",
            "     | > loss_duration: 1.03462  (0.99082)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.12057  (23.61833)\n",
            "     | > grad_norm_1: 579.11438  (618.40924)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.64350  (0.54226)\n",
            "     | > loader_time: 0.00430  (0.00602)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.79875  (2.79875)\n",
            "     | > loss_disc_real_0: 0.42672  (0.42672)\n",
            "     | > loss_disc_real_1: 0.15897  (0.15897)\n",
            "     | > loss_disc_real_2: 0.22213  (0.22213)\n",
            "     | > loss_disc_real_3: 0.23329  (0.23329)\n",
            "     | > loss_disc_real_4: 0.17713  (0.17713)\n",
            "     | > loss_disc_real_5: 0.23363  (0.23363)\n",
            "     | > loss_0: 2.79875  (2.79875)\n",
            "     | > loss_gen: 2.03543  (2.03543)\n",
            "     | > loss_kl: 7.65118  (7.65118)\n",
            "     | > loss_feat: 3.90519  (3.90519)\n",
            "     | > loss_mel: 16.08723  (16.08723)\n",
            "     | > loss_duration: 2.37497  (2.37497)\n",
            "     | > loss_1: 32.05400  (32.05400)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.36210 \u001b[0m(+0.01942)\n",
            "     | > avg_loss_disc:\u001b[92m 2.79875 \u001b[0m(-0.06490)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.42672 \u001b[0m(-0.07590)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.15897 \u001b[0m(-0.02021)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.22213 \u001b[0m(+0.00330)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.23329 \u001b[0m(+0.01469)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.17713 \u001b[0m(-0.00171)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.23363 \u001b[0m(+0.05204)\n",
            "     | > avg_loss_0:\u001b[92m 2.79875 \u001b[0m(-0.06490)\n",
            "     | > avg_loss_gen:\u001b[91m 2.03543 \u001b[0m(+0.16144)\n",
            "     | > avg_loss_kl:\u001b[92m 7.65118 \u001b[0m(-0.01482)\n",
            "     | > avg_loss_feat:\u001b[92m 3.90519 \u001b[0m(-0.43736)\n",
            "     | > avg_loss_mel:\u001b[91m 16.08723 \u001b[0m(+0.54174)\n",
            "     | > avg_loss_duration:\u001b[91m 2.37497 \u001b[0m(+0.00261)\n",
            "     | > avg_loss_1:\u001b[91m 32.05400 \u001b[0m(+0.25361)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 310/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:20:12) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 9/49 -- GLOBAL_STEP: 1051200\u001b[0m\n",
            "     | > loss_disc: 2.37805  (2.42762)\n",
            "     | > loss_disc_real_0: 0.04746  (0.08151)\n",
            "     | > loss_disc_real_1: 0.26681  (0.22034)\n",
            "     | > loss_disc_real_2: 0.23953  (0.22775)\n",
            "     | > loss_disc_real_3: 0.22106  (0.23600)\n",
            "     | > loss_disc_real_4: 0.22656  (0.24441)\n",
            "     | > loss_disc_real_5: 0.26845  (0.24836)\n",
            "     | > loss_0: 2.37805  (2.42762)\n",
            "     | > grad_norm_0: 7.90822  (9.17563)\n",
            "     | > loss_gen: 2.52880  (2.51115)\n",
            "     | > loss_kl: 0.79003  (0.92992)\n",
            "     | > loss_feat: 5.20946  (5.18084)\n",
            "     | > loss_mel: 14.59068  (13.97560)\n",
            "     | > loss_duration: 1.00077  (0.99360)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.11975  (23.59111)\n",
            "     | > grad_norm_1: 453.07587  (307.00937)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.49640  (0.52851)\n",
            "     | > loader_time: 0.01100  (0.00688)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 3.00043  (3.00043)\n",
            "     | > loss_disc_real_0: 0.54406  (0.54406)\n",
            "     | > loss_disc_real_1: 0.22464  (0.22464)\n",
            "     | > loss_disc_real_2: 0.19676  (0.19676)\n",
            "     | > loss_disc_real_3: 0.26747  (0.26747)\n",
            "     | > loss_disc_real_4: 0.19579  (0.19579)\n",
            "     | > loss_disc_real_5: 0.18402  (0.18402)\n",
            "     | > loss_0: 3.00043  (3.00043)\n",
            "     | > loss_gen: 2.01799  (2.01799)\n",
            "     | > loss_kl: 7.43682  (7.43682)\n",
            "     | > loss_feat: 4.44982  (4.44982)\n",
            "     | > loss_mel: 15.84882  (15.84882)\n",
            "     | > loss_duration: 2.35680  (2.35680)\n",
            "     | > loss_1: 32.11025  (32.11025)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.36093 \u001b[0m(-0.00117)\n",
            "     | > avg_loss_disc:\u001b[91m 3.00043 \u001b[0m(+0.20168)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.54406 \u001b[0m(+0.11734)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.22464 \u001b[0m(+0.06567)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.19676 \u001b[0m(-0.02536)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.26747 \u001b[0m(+0.03419)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.19579 \u001b[0m(+0.01866)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.18402 \u001b[0m(-0.04960)\n",
            "     | > avg_loss_0:\u001b[91m 3.00043 \u001b[0m(+0.20168)\n",
            "     | > avg_loss_gen:\u001b[92m 2.01799 \u001b[0m(-0.01744)\n",
            "     | > avg_loss_kl:\u001b[92m 7.43682 \u001b[0m(-0.21436)\n",
            "     | > avg_loss_feat:\u001b[91m 4.44982 \u001b[0m(+0.54463)\n",
            "     | > avg_loss_mel:\u001b[92m 15.84882 \u001b[0m(-0.23841)\n",
            "     | > avg_loss_duration:\u001b[92m 2.35680 \u001b[0m(-0.01817)\n",
            "     | > avg_loss_1:\u001b[91m 32.11025 \u001b[0m(+0.05625)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 311/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:20:50) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 10/49 -- GLOBAL_STEP: 1051250\u001b[0m\n",
            "     | > loss_disc: 2.34407  (2.40190)\n",
            "     | > loss_disc_real_0: 0.07047  (0.07993)\n",
            "     | > loss_disc_real_1: 0.25366  (0.22544)\n",
            "     | > loss_disc_real_2: 0.20786  (0.22526)\n",
            "     | > loss_disc_real_3: 0.22084  (0.22636)\n",
            "     | > loss_disc_real_4: 0.24216  (0.24112)\n",
            "     | > loss_disc_real_5: 0.22337  (0.23846)\n",
            "     | > loss_0: 2.34407  (2.40190)\n",
            "     | > grad_norm_0: 5.99423  (7.44736)\n",
            "     | > loss_gen: 2.51977  (2.49461)\n",
            "     | > loss_kl: 1.08022  (0.99750)\n",
            "     | > loss_feat: 5.34051  (5.41722)\n",
            "     | > loss_mel: 14.20128  (13.64037)\n",
            "     | > loss_duration: 1.02939  (0.99274)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.17118  (23.54244)\n",
            "     | > grad_norm_1: 119.65759  (206.63284)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.65460  (0.58828)\n",
            "     | > loader_time: 0.00560  (0.00829)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.78307  (2.78307)\n",
            "     | > loss_disc_real_0: 0.56846  (0.56846)\n",
            "     | > loss_disc_real_1: 0.13610  (0.13610)\n",
            "     | > loss_disc_real_2: 0.14774  (0.14774)\n",
            "     | > loss_disc_real_3: 0.21283  (0.21283)\n",
            "     | > loss_disc_real_4: 0.22929  (0.22929)\n",
            "     | > loss_disc_real_5: 0.21778  (0.21778)\n",
            "     | > loss_0: 2.78307  (2.78307)\n",
            "     | > loss_gen: 2.17699  (2.17699)\n",
            "     | > loss_kl: 7.46757  (7.46757)\n",
            "     | > loss_feat: 4.69236  (4.69236)\n",
            "     | > loss_mel: 16.22918  (16.22918)\n",
            "     | > loss_duration: 2.39947  (2.39947)\n",
            "     | > loss_1: 32.96557  (32.96557)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.36599 \u001b[0m(+0.00506)\n",
            "     | > avg_loss_disc:\u001b[92m 2.78307 \u001b[0m(-0.21736)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.56846 \u001b[0m(+0.02440)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.13610 \u001b[0m(-0.08854)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.14774 \u001b[0m(-0.04902)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.21283 \u001b[0m(-0.05465)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.22929 \u001b[0m(+0.03350)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.21778 \u001b[0m(+0.03376)\n",
            "     | > avg_loss_0:\u001b[92m 2.78307 \u001b[0m(-0.21736)\n",
            "     | > avg_loss_gen:\u001b[91m 2.17699 \u001b[0m(+0.15900)\n",
            "     | > avg_loss_kl:\u001b[91m 7.46757 \u001b[0m(+0.03075)\n",
            "     | > avg_loss_feat:\u001b[91m 4.69236 \u001b[0m(+0.24254)\n",
            "     | > avg_loss_mel:\u001b[91m 16.22918 \u001b[0m(+0.38036)\n",
            "     | > avg_loss_duration:\u001b[91m 2.39947 \u001b[0m(+0.04267)\n",
            "     | > avg_loss_1:\u001b[91m 32.96557 \u001b[0m(+0.85532)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 312/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:21:29) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 11/49 -- GLOBAL_STEP: 1051300\u001b[0m\n",
            "     | > loss_disc: 2.49194  (2.44846)\n",
            "     | > loss_disc_real_0: 0.10300  (0.08056)\n",
            "     | > loss_disc_real_1: 0.18239  (0.22362)\n",
            "     | > loss_disc_real_2: 0.22685  (0.23105)\n",
            "     | > loss_disc_real_3: 0.24220  (0.23171)\n",
            "     | > loss_disc_real_4: 0.25039  (0.23959)\n",
            "     | > loss_disc_real_5: 0.26991  (0.24155)\n",
            "     | > loss_0: 2.49194  (2.44846)\n",
            "     | > grad_norm_0: 12.82809  (9.55016)\n",
            "     | > loss_gen: 2.52206  (2.50516)\n",
            "     | > loss_kl: 1.06850  (0.98442)\n",
            "     | > loss_feat: 5.40867  (5.21869)\n",
            "     | > loss_mel: 14.25042  (13.79554)\n",
            "     | > loss_duration: 0.97420  (0.97730)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.22385  (23.48111)\n",
            "     | > grad_norm_1: 360.28210  (458.84244)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.52480  (0.52044)\n",
            "     | > loader_time: 0.00480  (0.00601)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.88689  (2.88689)\n",
            "     | > loss_disc_real_0: 0.46477  (0.46477)\n",
            "     | > loss_disc_real_1: 0.26476  (0.26476)\n",
            "     | > loss_disc_real_2: 0.19320  (0.19320)\n",
            "     | > loss_disc_real_3: 0.25682  (0.25682)\n",
            "     | > loss_disc_real_4: 0.23648  (0.23648)\n",
            "     | > loss_disc_real_5: 0.25958  (0.25958)\n",
            "     | > loss_0: 2.88689  (2.88689)\n",
            "     | > loss_gen: 2.35202  (2.35202)\n",
            "     | > loss_kl: 7.92006  (7.92006)\n",
            "     | > loss_feat: 4.19872  (4.19872)\n",
            "     | > loss_mel: 15.94575  (15.94575)\n",
            "     | > loss_duration: 2.36429  (2.36429)\n",
            "     | > loss_1: 32.78083  (32.78083)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.36907 \u001b[0m(+0.00308)\n",
            "     | > avg_loss_disc:\u001b[91m 2.88689 \u001b[0m(+0.10381)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.46477 \u001b[0m(-0.10369)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.26476 \u001b[0m(+0.12866)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.19320 \u001b[0m(+0.04546)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.25682 \u001b[0m(+0.04399)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.23648 \u001b[0m(+0.00719)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.25958 \u001b[0m(+0.04180)\n",
            "     | > avg_loss_0:\u001b[91m 2.88689 \u001b[0m(+0.10381)\n",
            "     | > avg_loss_gen:\u001b[91m 2.35202 \u001b[0m(+0.17503)\n",
            "     | > avg_loss_kl:\u001b[91m 7.92006 \u001b[0m(+0.45249)\n",
            "     | > avg_loss_feat:\u001b[92m 4.19872 \u001b[0m(-0.49364)\n",
            "     | > avg_loss_mel:\u001b[92m 15.94575 \u001b[0m(-0.28343)\n",
            "     | > avg_loss_duration:\u001b[92m 2.36429 \u001b[0m(-0.03518)\n",
            "     | > avg_loss_1:\u001b[92m 32.78083 \u001b[0m(-0.18473)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 313/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:22:08) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 12/49 -- GLOBAL_STEP: 1051350\u001b[0m\n",
            "     | > loss_disc: 2.47771  (2.43660)\n",
            "     | > loss_disc_real_0: 0.14165  (0.06366)\n",
            "     | > loss_disc_real_1: 0.22940  (0.21728)\n",
            "     | > loss_disc_real_2: 0.20332  (0.22574)\n",
            "     | > loss_disc_real_3: 0.20331  (0.22993)\n",
            "     | > loss_disc_real_4: 0.26714  (0.24755)\n",
            "     | > loss_disc_real_5: 0.26007  (0.23535)\n",
            "     | > loss_0: 2.47771  (2.43660)\n",
            "     | > grad_norm_0: 8.59677  (5.45293)\n",
            "     | > loss_gen: 2.36403  (2.43389)\n",
            "     | > loss_kl: 0.88212  (0.96088)\n",
            "     | > loss_feat: 5.33886  (5.27962)\n",
            "     | > loss_mel: 13.80107  (13.76270)\n",
            "     | > loss_duration: 1.00862  (0.98481)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.39469  (23.42189)\n",
            "     | > grad_norm_1: 270.27383  (192.38635)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.51530  (0.60572)\n",
            "     | > loader_time: 0.00610  (0.00744)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.96236  (2.96236)\n",
            "     | > loss_disc_real_0: 0.45708  (0.45708)\n",
            "     | > loss_disc_real_1: 0.19192  (0.19192)\n",
            "     | > loss_disc_real_2: 0.27011  (0.27011)\n",
            "     | > loss_disc_real_3: 0.19503  (0.19503)\n",
            "     | > loss_disc_real_4: 0.23383  (0.23383)\n",
            "     | > loss_disc_real_5: 0.22906  (0.22906)\n",
            "     | > loss_0: 2.96236  (2.96236)\n",
            "     | > loss_gen: 2.08516  (2.08516)\n",
            "     | > loss_kl: 7.55065  (7.55065)\n",
            "     | > loss_feat: 4.28585  (4.28585)\n",
            "     | > loss_mel: 16.65142  (16.65142)\n",
            "     | > loss_duration: 2.37086  (2.37086)\n",
            "     | > loss_1: 32.94394  (32.94394)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.38191 \u001b[0m(+0.01284)\n",
            "     | > avg_loss_disc:\u001b[91m 2.96236 \u001b[0m(+0.07547)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.45708 \u001b[0m(-0.00769)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.19192 \u001b[0m(-0.07283)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.27011 \u001b[0m(+0.07691)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.19503 \u001b[0m(-0.06179)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.23383 \u001b[0m(-0.00266)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.22906 \u001b[0m(-0.03052)\n",
            "     | > avg_loss_0:\u001b[91m 2.96236 \u001b[0m(+0.07547)\n",
            "     | > avg_loss_gen:\u001b[92m 2.08516 \u001b[0m(-0.26686)\n",
            "     | > avg_loss_kl:\u001b[92m 7.55065 \u001b[0m(-0.36940)\n",
            "     | > avg_loss_feat:\u001b[91m 4.28585 \u001b[0m(+0.08713)\n",
            "     | > avg_loss_mel:\u001b[91m 16.65142 \u001b[0m(+0.70567)\n",
            "     | > avg_loss_duration:\u001b[91m 2.37086 \u001b[0m(+0.00657)\n",
            "     | > avg_loss_1:\u001b[91m 32.94394 \u001b[0m(+0.16311)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 314/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:22:48) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 13/49 -- GLOBAL_STEP: 1051400\u001b[0m\n",
            "     | > loss_disc: 2.36012  (2.39704)\n",
            "     | > loss_disc_real_0: 0.07088  (0.07503)\n",
            "     | > loss_disc_real_1: 0.22595  (0.22034)\n",
            "     | > loss_disc_real_2: 0.13453  (0.21512)\n",
            "     | > loss_disc_real_3: 0.18047  (0.22963)\n",
            "     | > loss_disc_real_4: 0.19920  (0.23261)\n",
            "     | > loss_disc_real_5: 0.19247  (0.22274)\n",
            "     | > loss_0: 2.36012  (2.39704)\n",
            "     | > grad_norm_0: 7.12349  (7.94388)\n",
            "     | > loss_gen: 2.20746  (2.42732)\n",
            "     | > loss_kl: 1.05719  (1.10909)\n",
            "     | > loss_feat: 5.50380  (5.23767)\n",
            "     | > loss_mel: 14.19390  (14.07649)\n",
            "     | > loss_duration: 0.93469  (0.97439)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.89703  (23.82497)\n",
            "     | > grad_norm_1: 645.14771  (496.46854)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.50950  (0.52440)\n",
            "     | > loader_time: 0.00550  (0.00643)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.78500  (2.78500)\n",
            "     | > loss_disc_real_0: 0.34600  (0.34600)\n",
            "     | > loss_disc_real_1: 0.26294  (0.26294)\n",
            "     | > loss_disc_real_2: 0.26399  (0.26399)\n",
            "     | > loss_disc_real_3: 0.23803  (0.23803)\n",
            "     | > loss_disc_real_4: 0.24335  (0.24335)\n",
            "     | > loss_disc_real_5: 0.17332  (0.17332)\n",
            "     | > loss_0: 2.78500  (2.78500)\n",
            "     | > loss_gen: 2.23953  (2.23953)\n",
            "     | > loss_kl: 7.25868  (7.25868)\n",
            "     | > loss_feat: 4.03150  (4.03150)\n",
            "     | > loss_mel: 15.46871  (15.46871)\n",
            "     | > loss_duration: 2.39072  (2.39072)\n",
            "     | > loss_1: 31.38914  (31.38914)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.36996 \u001b[0m(-0.01195)\n",
            "     | > avg_loss_disc:\u001b[92m 2.78500 \u001b[0m(-0.17736)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.34600 \u001b[0m(-0.11108)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.26294 \u001b[0m(+0.07102)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.26399 \u001b[0m(-0.00613)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.23803 \u001b[0m(+0.04300)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.24335 \u001b[0m(+0.00953)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.17332 \u001b[0m(-0.05574)\n",
            "     | > avg_loss_0:\u001b[92m 2.78500 \u001b[0m(-0.17736)\n",
            "     | > avg_loss_gen:\u001b[91m 2.23953 \u001b[0m(+0.15436)\n",
            "     | > avg_loss_kl:\u001b[92m 7.25868 \u001b[0m(-0.29198)\n",
            "     | > avg_loss_feat:\u001b[92m 4.03150 \u001b[0m(-0.25434)\n",
            "     | > avg_loss_mel:\u001b[92m 15.46871 \u001b[0m(-1.18271)\n",
            "     | > avg_loss_duration:\u001b[91m 2.39072 \u001b[0m(+0.01986)\n",
            "     | > avg_loss_1:\u001b[92m 31.38914 \u001b[0m(-1.55480)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 315/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:23:25) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 14/49 -- GLOBAL_STEP: 1051450\u001b[0m\n",
            "     | > loss_disc: 2.39611  (2.44066)\n",
            "     | > loss_disc_real_0: 0.03376  (0.07373)\n",
            "     | > loss_disc_real_1: 0.16822  (0.21508)\n",
            "     | > loss_disc_real_2: 0.22402  (0.22105)\n",
            "     | > loss_disc_real_3: 0.25517  (0.24299)\n",
            "     | > loss_disc_real_4: 0.20650  (0.23962)\n",
            "     | > loss_disc_real_5: 0.20019  (0.24167)\n",
            "     | > loss_0: 2.39611  (2.44066)\n",
            "     | > grad_norm_0: 5.05133  (8.19046)\n",
            "     | > loss_gen: 2.14912  (2.39904)\n",
            "     | > loss_kl: 1.24327  (1.18870)\n",
            "     | > loss_feat: 5.73993  (5.08864)\n",
            "     | > loss_mel: 14.05113  (13.96956)\n",
            "     | > loss_duration: 0.96625  (0.98097)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.14970  (23.62690)\n",
            "     | > grad_norm_1: 248.63686  (412.67038)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.64610  (0.65884)\n",
            "     | > loader_time: 0.01410  (0.01599)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 3.07441  (3.07441)\n",
            "     | > loss_disc_real_0: 0.61100  (0.61100)\n",
            "     | > loss_disc_real_1: 0.17784  (0.17784)\n",
            "     | > loss_disc_real_2: 0.22621  (0.22621)\n",
            "     | > loss_disc_real_3: 0.23781  (0.23781)\n",
            "     | > loss_disc_real_4: 0.26212  (0.26212)\n",
            "     | > loss_disc_real_5: 0.19808  (0.19808)\n",
            "     | > loss_0: 3.07441  (3.07441)\n",
            "     | > loss_gen: 2.08701  (2.08701)\n",
            "     | > loss_kl: 7.37666  (7.37666)\n",
            "     | > loss_feat: 3.99741  (3.99741)\n",
            "     | > loss_mel: 17.35809  (17.35809)\n",
            "     | > loss_duration: 2.41084  (2.41084)\n",
            "     | > loss_1: 33.23001  (33.23001)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.55813 \u001b[0m(+0.18817)\n",
            "     | > avg_loss_disc:\u001b[91m 3.07441 \u001b[0m(+0.28942)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.61100 \u001b[0m(+0.26500)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.17784 \u001b[0m(-0.08510)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.22621 \u001b[0m(-0.03778)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.23781 \u001b[0m(-0.00022)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.26212 \u001b[0m(+0.01877)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.19808 \u001b[0m(+0.02476)\n",
            "     | > avg_loss_0:\u001b[91m 3.07441 \u001b[0m(+0.28942)\n",
            "     | > avg_loss_gen:\u001b[92m 2.08701 \u001b[0m(-0.15251)\n",
            "     | > avg_loss_kl:\u001b[91m 7.37666 \u001b[0m(+0.11798)\n",
            "     | > avg_loss_feat:\u001b[92m 3.99741 \u001b[0m(-0.03409)\n",
            "     | > avg_loss_mel:\u001b[91m 17.35809 \u001b[0m(+1.88938)\n",
            "     | > avg_loss_duration:\u001b[91m 2.41084 \u001b[0m(+0.02012)\n",
            "     | > avg_loss_1:\u001b[91m 33.23001 \u001b[0m(+1.84087)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 316/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:24:04) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 15/49 -- GLOBAL_STEP: 1051500\u001b[0m\n",
            "     | > loss_disc: 2.34335  (2.43724)\n",
            "     | > loss_disc_real_0: 0.04017  (0.05449)\n",
            "     | > loss_disc_real_1: 0.20200  (0.22002)\n",
            "     | > loss_disc_real_2: 0.20921  (0.22885)\n",
            "     | > loss_disc_real_3: 0.22004  (0.23665)\n",
            "     | > loss_disc_real_4: 0.25856  (0.25060)\n",
            "     | > loss_disc_real_5: 0.26057  (0.24048)\n",
            "     | > loss_0: 2.34335  (2.43724)\n",
            "     | > grad_norm_0: 5.44350  (5.70492)\n",
            "     | > loss_gen: 2.35047  (2.39630)\n",
            "     | > loss_kl: 0.89842  (1.08053)\n",
            "     | > loss_feat: 5.40459  (5.26818)\n",
            "     | > loss_mel: 14.45798  (13.97505)\n",
            "     | > loss_duration: 1.01733  (0.99568)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.12879  (23.71576)\n",
            "     | > grad_norm_1: 602.41083  (239.33813)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.65700  (0.55244)\n",
            "     | > loader_time: 0.00480  (0.00758)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.74244  (2.74244)\n",
            "     | > loss_disc_real_0: 0.38747  (0.38747)\n",
            "     | > loss_disc_real_1: 0.24401  (0.24401)\n",
            "     | > loss_disc_real_2: 0.24273  (0.24273)\n",
            "     | > loss_disc_real_3: 0.23575  (0.23575)\n",
            "     | > loss_disc_real_4: 0.25904  (0.25904)\n",
            "     | > loss_disc_real_5: 0.20010  (0.20010)\n",
            "     | > loss_0: 2.74244  (2.74244)\n",
            "     | > loss_gen: 2.28364  (2.28364)\n",
            "     | > loss_kl: 7.26852  (7.26852)\n",
            "     | > loss_feat: 4.23029  (4.23029)\n",
            "     | > loss_mel: 15.03239  (15.03239)\n",
            "     | > loss_duration: 2.37178  (2.37178)\n",
            "     | > loss_1: 31.18662  (31.18662)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.37028 \u001b[0m(-0.18785)\n",
            "     | > avg_loss_disc:\u001b[92m 2.74244 \u001b[0m(-0.33197)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.38747 \u001b[0m(-0.22353)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.24401 \u001b[0m(+0.06617)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.24273 \u001b[0m(+0.01652)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.23575 \u001b[0m(-0.00206)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.25904 \u001b[0m(-0.00307)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.20010 \u001b[0m(+0.00201)\n",
            "     | > avg_loss_0:\u001b[92m 2.74244 \u001b[0m(-0.33197)\n",
            "     | > avg_loss_gen:\u001b[91m 2.28364 \u001b[0m(+0.19662)\n",
            "     | > avg_loss_kl:\u001b[92m 7.26852 \u001b[0m(-0.10814)\n",
            "     | > avg_loss_feat:\u001b[91m 4.23029 \u001b[0m(+0.23288)\n",
            "     | > avg_loss_mel:\u001b[92m 15.03239 \u001b[0m(-2.32570)\n",
            "     | > avg_loss_duration:\u001b[92m 2.37178 \u001b[0m(-0.03906)\n",
            "     | > avg_loss_1:\u001b[92m 31.18662 \u001b[0m(-2.04339)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 317/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:24:42) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 16/49 -- GLOBAL_STEP: 1051550\u001b[0m\n",
            "     | > loss_disc: 2.35615  (2.40071)\n",
            "     | > loss_disc_real_0: 0.04191  (0.06148)\n",
            "     | > loss_disc_real_1: 0.20145  (0.21324)\n",
            "     | > loss_disc_real_2: 0.20838  (0.22156)\n",
            "     | > loss_disc_real_3: 0.29470  (0.23475)\n",
            "     | > loss_disc_real_4: 0.23745  (0.23650)\n",
            "     | > loss_disc_real_5: 0.25009  (0.23861)\n",
            "     | > loss_0: 2.35615  (2.40071)\n",
            "     | > grad_norm_0: 8.19031  (8.32104)\n",
            "     | > loss_gen: 2.52818  (2.44405)\n",
            "     | > loss_kl: 1.34307  (1.04210)\n",
            "     | > loss_feat: 5.06818  (5.36098)\n",
            "     | > loss_mel: 14.59268  (13.91743)\n",
            "     | > loss_duration: 1.02248  (0.99142)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.55458  (23.75599)\n",
            "     | > grad_norm_1: 1028.61743  (544.60925)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.66990  (0.60168)\n",
            "     | > loader_time: 0.00570  (0.00767)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 3.15515  (3.15515)\n",
            "     | > loss_disc_real_0: 0.64083  (0.64083)\n",
            "     | > loss_disc_real_1: 0.28627  (0.28627)\n",
            "     | > loss_disc_real_2: 0.22481  (0.22481)\n",
            "     | > loss_disc_real_3: 0.24329  (0.24329)\n",
            "     | > loss_disc_real_4: 0.20938  (0.20938)\n",
            "     | > loss_disc_real_5: 0.25204  (0.25204)\n",
            "     | > loss_0: 3.15515  (3.15515)\n",
            "     | > loss_gen: 2.15343  (2.15343)\n",
            "     | > loss_kl: 6.97555  (6.97555)\n",
            "     | > loss_feat: 4.57684  (4.57684)\n",
            "     | > loss_mel: 15.55763  (15.55763)\n",
            "     | > loss_duration: 2.42873  (2.42873)\n",
            "     | > loss_1: 31.69217  (31.69217)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.48245 \u001b[0m(+0.11217)\n",
            "     | > avg_loss_disc:\u001b[91m 3.15515 \u001b[0m(+0.41271)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.64083 \u001b[0m(+0.25336)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.28627 \u001b[0m(+0.04226)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.22481 \u001b[0m(-0.01792)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.24329 \u001b[0m(+0.00754)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.20938 \u001b[0m(-0.04967)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.25204 \u001b[0m(+0.05194)\n",
            "     | > avg_loss_0:\u001b[91m 3.15515 \u001b[0m(+0.41271)\n",
            "     | > avg_loss_gen:\u001b[92m 2.15343 \u001b[0m(-0.13021)\n",
            "     | > avg_loss_kl:\u001b[92m 6.97555 \u001b[0m(-0.29298)\n",
            "     | > avg_loss_feat:\u001b[91m 4.57684 \u001b[0m(+0.34655)\n",
            "     | > avg_loss_mel:\u001b[91m 15.55763 \u001b[0m(+0.52524)\n",
            "     | > avg_loss_duration:\u001b[91m 2.42873 \u001b[0m(+0.05695)\n",
            "     | > avg_loss_1:\u001b[91m 31.69217 \u001b[0m(+0.50555)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 318/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:25:20) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 17/49 -- GLOBAL_STEP: 1051600\u001b[0m\n",
            "     | > loss_disc: 2.44126  (2.44986)\n",
            "     | > loss_disc_real_0: 0.06721  (0.06938)\n",
            "     | > loss_disc_real_1: 0.25760  (0.22618)\n",
            "     | > loss_disc_real_2: 0.24850  (0.22835)\n",
            "     | > loss_disc_real_3: 0.24737  (0.23577)\n",
            "     | > loss_disc_real_4: 0.25216  (0.25495)\n",
            "     | > loss_disc_real_5: 0.22998  (0.24033)\n",
            "     | > loss_0: 2.44126  (2.44986)\n",
            "     | > grad_norm_0: 8.06217  (8.69517)\n",
            "     | > loss_gen: 2.39293  (2.43286)\n",
            "     | > loss_kl: 0.75543  (1.11609)\n",
            "     | > loss_feat: 5.30691  (5.31177)\n",
            "     | > loss_mel: 14.57810  (13.93767)\n",
            "     | > loss_duration: 1.00617  (1.00432)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.03953  (23.80271)\n",
            "     | > grad_norm_1: 402.90903  (403.00565)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.69240  (0.55609)\n",
            "     | > loader_time: 0.02060  (0.00843)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.68113  (2.68113)\n",
            "     | > loss_disc_real_0: 0.36752  (0.36752)\n",
            "     | > loss_disc_real_1: 0.18353  (0.18353)\n",
            "     | > loss_disc_real_2: 0.16628  (0.16628)\n",
            "     | > loss_disc_real_3: 0.19157  (0.19157)\n",
            "     | > loss_disc_real_4: 0.19825  (0.19825)\n",
            "     | > loss_disc_real_5: 0.17520  (0.17520)\n",
            "     | > loss_0: 2.68113  (2.68113)\n",
            "     | > loss_gen: 1.92526  (1.92526)\n",
            "     | > loss_kl: 7.83588  (7.83588)\n",
            "     | > loss_feat: 4.35233  (4.35233)\n",
            "     | > loss_mel: 15.03690  (15.03690)\n",
            "     | > loss_duration: 2.36238  (2.36238)\n",
            "     | > loss_1: 31.51275  (31.51275)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.38591 \u001b[0m(-0.09654)\n",
            "     | > avg_loss_disc:\u001b[92m 2.68113 \u001b[0m(-0.47401)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.36752 \u001b[0m(-0.27331)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.18353 \u001b[0m(-0.10274)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.16628 \u001b[0m(-0.05853)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.19157 \u001b[0m(-0.05172)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.19825 \u001b[0m(-0.01112)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.17520 \u001b[0m(-0.07684)\n",
            "     | > avg_loss_0:\u001b[92m 2.68113 \u001b[0m(-0.47401)\n",
            "     | > avg_loss_gen:\u001b[92m 1.92526 \u001b[0m(-0.22817)\n",
            "     | > avg_loss_kl:\u001b[91m 7.83588 \u001b[0m(+0.86033)\n",
            "     | > avg_loss_feat:\u001b[92m 4.35233 \u001b[0m(-0.22451)\n",
            "     | > avg_loss_mel:\u001b[92m 15.03690 \u001b[0m(-0.52074)\n",
            "     | > avg_loss_duration:\u001b[92m 2.36238 \u001b[0m(-0.06635)\n",
            "     | > avg_loss_1:\u001b[92m 31.51275 \u001b[0m(-0.17942)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 319/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:25:57) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 18/49 -- GLOBAL_STEP: 1051650\u001b[0m\n",
            "     | > loss_disc: 2.51560  (2.44411)\n",
            "     | > loss_disc_real_0: 0.09229  (0.07217)\n",
            "     | > loss_disc_real_1: 0.22273  (0.21275)\n",
            "     | > loss_disc_real_2: 0.23101  (0.22304)\n",
            "     | > loss_disc_real_3: 0.23442  (0.23466)\n",
            "     | > loss_disc_real_4: 0.24163  (0.24282)\n",
            "     | > loss_disc_real_5: 0.24462  (0.24315)\n",
            "     | > loss_0: 2.51560  (2.44411)\n",
            "     | > grad_norm_0: 7.70966  (7.61541)\n",
            "     | > loss_gen: 2.52274  (2.42521)\n",
            "     | > loss_kl: 1.30530  (1.04376)\n",
            "     | > loss_feat: 4.92430  (5.19991)\n",
            "     | > loss_mel: 14.36669  (14.08655)\n",
            "     | > loss_duration: 0.98291  (1.00375)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.10194  (23.75917)\n",
            "     | > grad_norm_1: 660.54205  (322.24661)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.52310  (0.56555)\n",
            "     | > loader_time: 0.01140  (0.00900)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.90564  (2.90564)\n",
            "     | > loss_disc_real_0: 0.36674  (0.36674)\n",
            "     | > loss_disc_real_1: 0.26704  (0.26704)\n",
            "     | > loss_disc_real_2: 0.22438  (0.22438)\n",
            "     | > loss_disc_real_3: 0.24029  (0.24029)\n",
            "     | > loss_disc_real_4: 0.25519  (0.25519)\n",
            "     | > loss_disc_real_5: 0.30390  (0.30390)\n",
            "     | > loss_0: 2.90564  (2.90564)\n",
            "     | > loss_gen: 2.09439  (2.09439)\n",
            "     | > loss_kl: 7.26364  (7.26364)\n",
            "     | > loss_feat: 3.57943  (3.57943)\n",
            "     | > loss_mel: 16.10116  (16.10116)\n",
            "     | > loss_duration: 2.35240  (2.35240)\n",
            "     | > loss_1: 31.39102  (31.39102)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.57060 \u001b[0m(+0.18469)\n",
            "     | > avg_loss_disc:\u001b[91m 2.90564 \u001b[0m(+0.22450)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.36674 \u001b[0m(-0.00078)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.26704 \u001b[0m(+0.08351)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.22438 \u001b[0m(+0.05810)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.24029 \u001b[0m(+0.04872)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.25519 \u001b[0m(+0.05693)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.30390 \u001b[0m(+0.12870)\n",
            "     | > avg_loss_0:\u001b[91m 2.90564 \u001b[0m(+0.22450)\n",
            "     | > avg_loss_gen:\u001b[91m 2.09439 \u001b[0m(+0.16913)\n",
            "     | > avg_loss_kl:\u001b[92m 7.26364 \u001b[0m(-0.57224)\n",
            "     | > avg_loss_feat:\u001b[92m 3.57943 \u001b[0m(-0.77289)\n",
            "     | > avg_loss_mel:\u001b[91m 16.10116 \u001b[0m(+1.06426)\n",
            "     | > avg_loss_duration:\u001b[92m 2.35240 \u001b[0m(-0.00998)\n",
            "     | > avg_loss_1:\u001b[92m 31.39102 \u001b[0m(-0.12173)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 320/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:26:36) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 19/49 -- GLOBAL_STEP: 1051700\u001b[0m\n",
            "     | > loss_disc: 2.33678  (2.41203)\n",
            "     | > loss_disc_real_0: 0.09196  (0.06982)\n",
            "     | > loss_disc_real_1: 0.19448  (0.21176)\n",
            "     | > loss_disc_real_2: 0.20021  (0.22672)\n",
            "     | > loss_disc_real_3: 0.19790  (0.22988)\n",
            "     | > loss_disc_real_4: 0.23239  (0.24263)\n",
            "     | > loss_disc_real_5: 0.19952  (0.22892)\n",
            "     | > loss_0: 2.33678  (2.41203)\n",
            "     | > grad_norm_0: 5.21816  (6.49314)\n",
            "     | > loss_gen: 2.41195  (2.39749)\n",
            "     | > loss_kl: 1.56216  (1.12029)\n",
            "     | > loss_feat: 5.67875  (5.18980)\n",
            "     | > loss_mel: 13.99942  (13.90418)\n",
            "     | > loss_duration: 1.00611  (1.00108)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.65840  (23.61285)\n",
            "     | > grad_norm_1: 406.82584  (177.81290)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.79640  (0.62279)\n",
            "     | > loader_time: 0.01130  (0.00732)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.93446  (2.93446)\n",
            "     | > loss_disc_real_0: 0.38547  (0.38547)\n",
            "     | > loss_disc_real_1: 0.23677  (0.23677)\n",
            "     | > loss_disc_real_2: 0.25836  (0.25836)\n",
            "     | > loss_disc_real_3: 0.23746  (0.23746)\n",
            "     | > loss_disc_real_4: 0.27133  (0.27133)\n",
            "     | > loss_disc_real_5: 0.26373  (0.26373)\n",
            "     | > loss_0: 2.93446  (2.93446)\n",
            "     | > loss_gen: 2.08852  (2.08852)\n",
            "     | > loss_kl: 7.24426  (7.24426)\n",
            "     | > loss_feat: 3.46976  (3.46976)\n",
            "     | > loss_mel: 16.01254  (16.01254)\n",
            "     | > loss_duration: 2.41709  (2.41709)\n",
            "     | > loss_1: 31.23218  (31.23218)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.37240 \u001b[0m(-0.19820)\n",
            "     | > avg_loss_disc:\u001b[91m 2.93446 \u001b[0m(+0.02882)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.38547 \u001b[0m(+0.01873)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.23677 \u001b[0m(-0.03027)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.25836 \u001b[0m(+0.03398)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.23746 \u001b[0m(-0.00283)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.27133 \u001b[0m(+0.01614)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.26373 \u001b[0m(-0.04016)\n",
            "     | > avg_loss_0:\u001b[91m 2.93446 \u001b[0m(+0.02882)\n",
            "     | > avg_loss_gen:\u001b[92m 2.08852 \u001b[0m(-0.00587)\n",
            "     | > avg_loss_kl:\u001b[92m 7.24426 \u001b[0m(-0.01938)\n",
            "     | > avg_loss_feat:\u001b[92m 3.46976 \u001b[0m(-0.10967)\n",
            "     | > avg_loss_mel:\u001b[92m 16.01254 \u001b[0m(-0.08861)\n",
            "     | > avg_loss_duration:\u001b[91m 2.41709 \u001b[0m(+0.06469)\n",
            "     | > avg_loss_1:\u001b[92m 31.23218 \u001b[0m(-0.15884)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 321/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:27:16) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 20/49 -- GLOBAL_STEP: 1051750\u001b[0m\n",
            "     | > loss_disc: 2.44044  (2.40301)\n",
            "     | > loss_disc_real_0: 0.09599  (0.06920)\n",
            "     | > loss_disc_real_1: 0.25295  (0.21191)\n",
            "     | > loss_disc_real_2: 0.24391  (0.21788)\n",
            "     | > loss_disc_real_3: 0.18847  (0.22852)\n",
            "     | > loss_disc_real_4: 0.21280  (0.23444)\n",
            "     | > loss_disc_real_5: 0.19611  (0.23727)\n",
            "     | > loss_0: 2.44044  (2.40301)\n",
            "     | > grad_norm_0: 4.64186  (6.45339)\n",
            "     | > loss_gen: 2.34261  (2.40159)\n",
            "     | > loss_kl: 1.43894  (1.18623)\n",
            "     | > loss_feat: 5.21875  (5.17915)\n",
            "     | > loss_mel: 14.98052  (14.07115)\n",
            "     | > loss_duration: 1.05430  (0.98996)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 25.03511  (23.82809)\n",
            "     | > grad_norm_1: 590.26923  (320.49496)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 1.34440  (0.59054)\n",
            "     | > loader_time: 0.01760  (0.00756)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.89230  (2.89230)\n",
            "     | > loss_disc_real_0: 0.48879  (0.48879)\n",
            "     | > loss_disc_real_1: 0.20390  (0.20390)\n",
            "     | > loss_disc_real_2: 0.19134  (0.19134)\n",
            "     | > loss_disc_real_3: 0.19129  (0.19129)\n",
            "     | > loss_disc_real_4: 0.18253  (0.18253)\n",
            "     | > loss_disc_real_5: 0.19741  (0.19741)\n",
            "     | > loss_0: 2.89230  (2.89230)\n",
            "     | > loss_gen: 2.03190  (2.03190)\n",
            "     | > loss_kl: 7.91808  (7.91808)\n",
            "     | > loss_feat: 4.58899  (4.58899)\n",
            "     | > loss_mel: 15.47541  (15.47541)\n",
            "     | > loss_duration: 2.34335  (2.34335)\n",
            "     | > loss_1: 32.35772  (32.35772)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.37346 \u001b[0m(+0.00105)\n",
            "     | > avg_loss_disc:\u001b[92m 2.89230 \u001b[0m(-0.04216)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.48879 \u001b[0m(+0.10331)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.20390 \u001b[0m(-0.03287)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.19134 \u001b[0m(-0.06702)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.19129 \u001b[0m(-0.04617)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.18253 \u001b[0m(-0.08879)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.19741 \u001b[0m(-0.06632)\n",
            "     | > avg_loss_0:\u001b[92m 2.89230 \u001b[0m(-0.04216)\n",
            "     | > avg_loss_gen:\u001b[92m 2.03190 \u001b[0m(-0.05663)\n",
            "     | > avg_loss_kl:\u001b[91m 7.91808 \u001b[0m(+0.67382)\n",
            "     | > avg_loss_feat:\u001b[91m 4.58899 \u001b[0m(+1.11922)\n",
            "     | > avg_loss_mel:\u001b[92m 15.47541 \u001b[0m(-0.53713)\n",
            "     | > avg_loss_duration:\u001b[92m 2.34335 \u001b[0m(-0.07374)\n",
            "     | > avg_loss_1:\u001b[91m 32.35772 \u001b[0m(+1.12554)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 322/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:27:55) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 21/49 -- GLOBAL_STEP: 1051800\u001b[0m\n",
            "     | > loss_disc: 2.34415  (2.39576)\n",
            "     | > loss_disc_real_0: 0.05033  (0.07507)\n",
            "     | > loss_disc_real_1: 0.25474  (0.21816)\n",
            "     | > loss_disc_real_2: 0.19976  (0.22221)\n",
            "     | > loss_disc_real_3: 0.21743  (0.22875)\n",
            "     | > loss_disc_real_4: 0.22306  (0.23863)\n",
            "     | > loss_disc_real_5: 0.22296  (0.23847)\n",
            "     | > loss_0: 2.34415  (2.39576)\n",
            "     | > grad_norm_0: 8.30924  (9.07707)\n",
            "     | > loss_gen: 2.31871  (2.44777)\n",
            "     | > loss_kl: 1.68401  (1.17407)\n",
            "     | > loss_feat: 5.74393  (5.33023)\n",
            "     | > loss_mel: 14.50443  (14.10400)\n",
            "     | > loss_duration: 1.01448  (1.00404)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 25.26556  (24.06011)\n",
            "     | > grad_norm_1: 432.12350  (491.87772)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.56230  (0.57655)\n",
            "     | > loader_time: 0.01030  (0.00700)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.85373  (2.85373)\n",
            "     | > loss_disc_real_0: 0.27622  (0.27622)\n",
            "     | > loss_disc_real_1: 0.14801  (0.14801)\n",
            "     | > loss_disc_real_2: 0.20107  (0.20107)\n",
            "     | > loss_disc_real_3: 0.13229  (0.13229)\n",
            "     | > loss_disc_real_4: 0.20760  (0.20760)\n",
            "     | > loss_disc_real_5: 0.23338  (0.23338)\n",
            "     | > loss_0: 2.85373  (2.85373)\n",
            "     | > loss_gen: 1.88513  (1.88513)\n",
            "     | > loss_kl: 7.56202  (7.56202)\n",
            "     | > loss_feat: 4.89478  (4.89478)\n",
            "     | > loss_mel: 15.65328  (15.65328)\n",
            "     | > loss_duration: 2.32539  (2.32539)\n",
            "     | > loss_1: 32.32061  (32.32061)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.37317 \u001b[0m(-0.00029)\n",
            "     | > avg_loss_disc:\u001b[92m 2.85373 \u001b[0m(-0.03857)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.27622 \u001b[0m(-0.21257)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.14801 \u001b[0m(-0.05589)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.20107 \u001b[0m(+0.00974)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.13229 \u001b[0m(-0.05900)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.20760 \u001b[0m(+0.02507)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.23338 \u001b[0m(+0.03597)\n",
            "     | > avg_loss_0:\u001b[92m 2.85373 \u001b[0m(-0.03857)\n",
            "     | > avg_loss_gen:\u001b[92m 1.88513 \u001b[0m(-0.14677)\n",
            "     | > avg_loss_kl:\u001b[92m 7.56202 \u001b[0m(-0.35606)\n",
            "     | > avg_loss_feat:\u001b[91m 4.89478 \u001b[0m(+0.30580)\n",
            "     | > avg_loss_mel:\u001b[91m 15.65328 \u001b[0m(+0.17787)\n",
            "     | > avg_loss_duration:\u001b[92m 2.32539 \u001b[0m(-0.01795)\n",
            "     | > avg_loss_1:\u001b[92m 32.32061 \u001b[0m(-0.03712)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 323/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:28:33) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 22/49 -- GLOBAL_STEP: 1051850\u001b[0m\n",
            "     | > loss_disc: 2.47000  (2.43168)\n",
            "     | > loss_disc_real_0: 0.06755  (0.07622)\n",
            "     | > loss_disc_real_1: 0.25671  (0.21640)\n",
            "     | > loss_disc_real_2: 0.22730  (0.22295)\n",
            "     | > loss_disc_real_3: 0.24213  (0.23649)\n",
            "     | > loss_disc_real_4: 0.24356  (0.24175)\n",
            "     | > loss_disc_real_5: 0.24733  (0.23743)\n",
            "     | > loss_0: 2.47000  (2.43168)\n",
            "     | > grad_norm_0: 5.72647  (7.78201)\n",
            "     | > loss_gen: 2.10541  (2.40315)\n",
            "     | > loss_kl: 1.51579  (1.14915)\n",
            "     | > loss_feat: 4.96550  (5.27919)\n",
            "     | > loss_mel: 14.67779  (14.22283)\n",
            "     | > loss_duration: 1.06888  (1.01012)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.33338  (24.06444)\n",
            "     | > grad_norm_1: 250.39241  (445.20172)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.63860  (0.52962)\n",
            "     | > loader_time: 0.00420  (0.00702)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.77181  (2.77181)\n",
            "     | > loss_disc_real_0: 0.36325  (0.36325)\n",
            "     | > loss_disc_real_1: 0.21940  (0.21940)\n",
            "     | > loss_disc_real_2: 0.21721  (0.21721)\n",
            "     | > loss_disc_real_3: 0.20406  (0.20406)\n",
            "     | > loss_disc_real_4: 0.20362  (0.20362)\n",
            "     | > loss_disc_real_5: 0.21193  (0.21193)\n",
            "     | > loss_0: 2.77181  (2.77181)\n",
            "     | > loss_gen: 2.11076  (2.11076)\n",
            "     | > loss_kl: 7.74453  (7.74453)\n",
            "     | > loss_feat: 4.42168  (4.42168)\n",
            "     | > loss_mel: 16.16595  (16.16595)\n",
            "     | > loss_duration: 2.39545  (2.39545)\n",
            "     | > loss_1: 32.83836  (32.83836)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.53926 \u001b[0m(+0.16609)\n",
            "     | > avg_loss_disc:\u001b[92m 2.77181 \u001b[0m(-0.08192)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.36325 \u001b[0m(+0.08703)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.21940 \u001b[0m(+0.07140)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.21721 \u001b[0m(+0.01614)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.20406 \u001b[0m(+0.07176)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.20362 \u001b[0m(-0.00398)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.21193 \u001b[0m(-0.02145)\n",
            "     | > avg_loss_0:\u001b[92m 2.77181 \u001b[0m(-0.08192)\n",
            "     | > avg_loss_gen:\u001b[91m 2.11076 \u001b[0m(+0.22563)\n",
            "     | > avg_loss_kl:\u001b[91m 7.74453 \u001b[0m(+0.18251)\n",
            "     | > avg_loss_feat:\u001b[92m 4.42168 \u001b[0m(-0.47310)\n",
            "     | > avg_loss_mel:\u001b[91m 16.16595 \u001b[0m(+0.51266)\n",
            "     | > avg_loss_duration:\u001b[91m 2.39545 \u001b[0m(+0.07006)\n",
            "     | > avg_loss_1:\u001b[91m 32.83836 \u001b[0m(+0.51775)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 324/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:29:09) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 23/49 -- GLOBAL_STEP: 1051900\u001b[0m\n",
            "     | > loss_disc: 2.31140  (2.40532)\n",
            "     | > loss_disc_real_0: 0.04219  (0.08316)\n",
            "     | > loss_disc_real_1: 0.19630  (0.21979)\n",
            "     | > loss_disc_real_2: 0.18752  (0.21877)\n",
            "     | > loss_disc_real_3: 0.20348  (0.23007)\n",
            "     | > loss_disc_real_4: 0.22493  (0.24121)\n",
            "     | > loss_disc_real_5: 0.29029  (0.23048)\n",
            "     | > loss_0: 2.31140  (2.40532)\n",
            "     | > grad_norm_0: 5.82073  (6.96116)\n",
            "     | > loss_gen: 2.28636  (2.37698)\n",
            "     | > loss_kl: 1.37589  (1.17119)\n",
            "     | > loss_feat: 5.04583  (5.17831)\n",
            "     | > loss_mel: 14.84885  (14.06908)\n",
            "     | > loss_duration: 1.07997  (1.00403)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.63690  (23.79960)\n",
            "     | > grad_norm_1: 96.82989  (154.23154)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.50860  (0.55774)\n",
            "     | > loader_time: 0.00550  (0.00647)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 3.16020  (3.16020)\n",
            "     | > loss_disc_real_0: 0.44881  (0.44881)\n",
            "     | > loss_disc_real_1: 0.31367  (0.31367)\n",
            "     | > loss_disc_real_2: 0.25645  (0.25645)\n",
            "     | > loss_disc_real_3: 0.19177  (0.19177)\n",
            "     | > loss_disc_real_4: 0.23412  (0.23412)\n",
            "     | > loss_disc_real_5: 0.28420  (0.28420)\n",
            "     | > loss_0: 3.16020  (3.16020)\n",
            "     | > loss_gen: 1.88494  (1.88494)\n",
            "     | > loss_kl: 7.35443  (7.35443)\n",
            "     | > loss_feat: 3.83827  (3.83827)\n",
            "     | > loss_mel: 15.42971  (15.42971)\n",
            "     | > loss_duration: 2.27151  (2.27151)\n",
            "     | > loss_1: 30.77887  (30.77887)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.36188 \u001b[0m(-0.17737)\n",
            "     | > avg_loss_disc:\u001b[91m 3.16020 \u001b[0m(+0.38839)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.44881 \u001b[0m(+0.08556)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.31367 \u001b[0m(+0.09427)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.25645 \u001b[0m(+0.03924)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.19177 \u001b[0m(-0.01228)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.23412 \u001b[0m(+0.03050)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.28420 \u001b[0m(+0.07227)\n",
            "     | > avg_loss_0:\u001b[91m 3.16020 \u001b[0m(+0.38839)\n",
            "     | > avg_loss_gen:\u001b[92m 1.88494 \u001b[0m(-0.22581)\n",
            "     | > avg_loss_kl:\u001b[92m 7.35443 \u001b[0m(-0.39009)\n",
            "     | > avg_loss_feat:\u001b[92m 3.83827 \u001b[0m(-0.58340)\n",
            "     | > avg_loss_mel:\u001b[92m 15.42971 \u001b[0m(-0.73623)\n",
            "     | > avg_loss_duration:\u001b[92m 2.27151 \u001b[0m(-0.12394)\n",
            "     | > avg_loss_1:\u001b[92m 30.77887 \u001b[0m(-2.05948)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 325/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:29:44) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 24/49 -- GLOBAL_STEP: 1051950\u001b[0m\n",
            "     | > loss_disc: 2.41972  (2.47058)\n",
            "     | > loss_disc_real_0: 0.03987  (0.08251)\n",
            "     | > loss_disc_real_1: 0.22732  (0.22516)\n",
            "     | > loss_disc_real_2: 0.22272  (0.21945)\n",
            "     | > loss_disc_real_3: 0.27900  (0.24311)\n",
            "     | > loss_disc_real_4: 0.24602  (0.24645)\n",
            "     | > loss_disc_real_5: 0.23115  (0.23768)\n",
            "     | > loss_0: 2.41972  (2.47058)\n",
            "     | > grad_norm_0: 7.19027  (9.73193)\n",
            "     | > loss_gen: 2.37865  (2.38377)\n",
            "     | > loss_kl: 1.03360  (1.15148)\n",
            "     | > loss_feat: 5.24442  (5.11692)\n",
            "     | > loss_mel: 14.66255  (14.22323)\n",
            "     | > loss_duration: 1.05713  (1.00678)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.37635  (23.88217)\n",
            "     | > grad_norm_1: 433.49948  (605.91534)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.64060  (0.56025)\n",
            "     | > loader_time: 0.00980  (0.00760)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.50562  (2.50562)\n",
            "     | > loss_disc_real_0: 0.19801  (0.19801)\n",
            "     | > loss_disc_real_1: 0.20933  (0.20933)\n",
            "     | > loss_disc_real_2: 0.26437  (0.26437)\n",
            "     | > loss_disc_real_3: 0.20366  (0.20366)\n",
            "     | > loss_disc_real_4: 0.22907  (0.22907)\n",
            "     | > loss_disc_real_5: 0.20787  (0.20787)\n",
            "     | > loss_0: 2.50562  (2.50562)\n",
            "     | > loss_gen: 2.33336  (2.33336)\n",
            "     | > loss_kl: 6.82482  (6.82482)\n",
            "     | > loss_feat: 4.97928  (4.97928)\n",
            "     | > loss_mel: 15.67665  (15.67665)\n",
            "     | > loss_duration: 2.27528  (2.27528)\n",
            "     | > loss_1: 32.08939  (32.08939)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.52138 \u001b[0m(+0.15950)\n",
            "     | > avg_loss_disc:\u001b[92m 2.50562 \u001b[0m(-0.65458)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.19801 \u001b[0m(-0.25080)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.20933 \u001b[0m(-0.10434)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.26437 \u001b[0m(+0.00792)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.20366 \u001b[0m(+0.01188)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.22907 \u001b[0m(-0.00504)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.20787 \u001b[0m(-0.07633)\n",
            "     | > avg_loss_0:\u001b[92m 2.50562 \u001b[0m(-0.65458)\n",
            "     | > avg_loss_gen:\u001b[91m 2.33336 \u001b[0m(+0.44842)\n",
            "     | > avg_loss_kl:\u001b[92m 6.82482 \u001b[0m(-0.52961)\n",
            "     | > avg_loss_feat:\u001b[91m 4.97928 \u001b[0m(+1.14100)\n",
            "     | > avg_loss_mel:\u001b[91m 15.67665 \u001b[0m(+0.24693)\n",
            "     | > avg_loss_duration:\u001b[91m 2.27528 \u001b[0m(+0.00377)\n",
            "     | > avg_loss_1:\u001b[91m 32.08939 \u001b[0m(+1.31052)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 326/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:30:22) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 25/49 -- GLOBAL_STEP: 1052000\u001b[0m\n",
            "     | > loss_disc: 2.30094  (2.45304)\n",
            "     | > loss_disc_real_0: 0.04262  (0.08035)\n",
            "     | > loss_disc_real_1: 0.20013  (0.21896)\n",
            "     | > loss_disc_real_2: 0.22788  (0.22108)\n",
            "     | > loss_disc_real_3: 0.21486  (0.23225)\n",
            "     | > loss_disc_real_4: 0.22589  (0.24361)\n",
            "     | > loss_disc_real_5: 0.21668  (0.24045)\n",
            "     | > loss_0: 2.30094  (2.45304)\n",
            "     | > grad_norm_0: 7.99394  (8.99640)\n",
            "     | > loss_gen: 2.50000  (2.37687)\n",
            "     | > loss_kl: 1.31195  (1.09809)\n",
            "     | > loss_feat: 5.20929  (5.17223)\n",
            "     | > loss_mel: 14.65610  (14.19206)\n",
            "     | > loss_duration: 0.99975  (1.01631)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.67708  (23.85556)\n",
            "     | > grad_norm_1: 1142.53247  (619.73712)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.51270  (0.55428)\n",
            "     | > loader_time: 0.01040  (0.00647)\n",
            "\n",
            "\n",
            " > CHECKPOINT : /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000/checkpoint_1052000.pth\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.77102  (2.77102)\n",
            "     | > loss_disc_real_0: 0.35381  (0.35381)\n",
            "     | > loss_disc_real_1: 0.18202  (0.18202)\n",
            "     | > loss_disc_real_2: 0.22728  (0.22728)\n",
            "     | > loss_disc_real_3: 0.20501  (0.20501)\n",
            "     | > loss_disc_real_4: 0.23361  (0.23361)\n",
            "     | > loss_disc_real_5: 0.24340  (0.24340)\n",
            "     | > loss_0: 2.77102  (2.77102)\n",
            "     | > loss_gen: 2.08878  (2.08878)\n",
            "     | > loss_kl: 7.01021  (7.01021)\n",
            "     | > loss_feat: 4.45178  (4.45178)\n",
            "     | > loss_mel: 15.88415  (15.88415)\n",
            "     | > loss_duration: 2.39119  (2.39119)\n",
            "     | > loss_1: 31.82611  (31.82611)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.50488 \u001b[0m(-0.01650)\n",
            "     | > avg_loss_disc:\u001b[91m 2.77102 \u001b[0m(+0.26541)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.35381 \u001b[0m(+0.15580)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.18202 \u001b[0m(-0.02731)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.22728 \u001b[0m(-0.03709)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.20501 \u001b[0m(+0.00135)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.23361 \u001b[0m(+0.00454)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.24340 \u001b[0m(+0.03553)\n",
            "     | > avg_loss_0:\u001b[91m 2.77102 \u001b[0m(+0.26541)\n",
            "     | > avg_loss_gen:\u001b[92m 2.08878 \u001b[0m(-0.24459)\n",
            "     | > avg_loss_kl:\u001b[91m 7.01021 \u001b[0m(+0.18539)\n",
            "     | > avg_loss_feat:\u001b[92m 4.45178 \u001b[0m(-0.52749)\n",
            "     | > avg_loss_mel:\u001b[91m 15.88415 \u001b[0m(+0.20750)\n",
            "     | > avg_loss_duration:\u001b[91m 2.39119 \u001b[0m(+0.11591)\n",
            "     | > avg_loss_1:\u001b[92m 31.82611 \u001b[0m(-0.26328)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 327/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:31:06) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 26/49 -- GLOBAL_STEP: 1052050\u001b[0m\n",
            "     | > loss_disc: 2.50205  (2.44859)\n",
            "     | > loss_disc_real_0: 0.08430  (0.07425)\n",
            "     | > loss_disc_real_1: 0.21868  (0.21900)\n",
            "     | > loss_disc_real_2: 0.25656  (0.22683)\n",
            "     | > loss_disc_real_3: 0.22455  (0.23746)\n",
            "     | > loss_disc_real_4: 0.23074  (0.24949)\n",
            "     | > loss_disc_real_5: 0.26056  (0.23829)\n",
            "     | > loss_0: 2.50205  (2.44859)\n",
            "     | > grad_norm_0: 7.58228  (8.76433)\n",
            "     | > loss_gen: 2.37118  (2.38788)\n",
            "     | > loss_kl: 0.59833  (1.12086)\n",
            "     | > loss_feat: 5.11791  (5.14921)\n",
            "     | > loss_mel: 14.64088  (14.35918)\n",
            "     | > loss_duration: 0.98845  (0.99869)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.71675  (24.01582)\n",
            "     | > grad_norm_1: 588.27869  (501.19586)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.53310  (0.66190)\n",
            "     | > loader_time: 0.00730  (0.00886)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.91653  (2.91653)\n",
            "     | > loss_disc_real_0: 0.48440  (0.48440)\n",
            "     | > loss_disc_real_1: 0.21691  (0.21691)\n",
            "     | > loss_disc_real_2: 0.15592  (0.15592)\n",
            "     | > loss_disc_real_3: 0.20558  (0.20558)\n",
            "     | > loss_disc_real_4: 0.18800  (0.18800)\n",
            "     | > loss_disc_real_5: 0.20959  (0.20959)\n",
            "     | > loss_0: 2.91653  (2.91653)\n",
            "     | > loss_gen: 1.82073  (1.82073)\n",
            "     | > loss_kl: 7.27351  (7.27351)\n",
            "     | > loss_feat: 3.91237  (3.91237)\n",
            "     | > loss_mel: 15.98418  (15.98418)\n",
            "     | > loss_duration: 2.37508  (2.37508)\n",
            "     | > loss_1: 31.36587  (31.36587)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.38311 \u001b[0m(-0.12177)\n",
            "     | > avg_loss_disc:\u001b[91m 2.91653 \u001b[0m(+0.14550)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.48440 \u001b[0m(+0.13059)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.21691 \u001b[0m(+0.03489)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.15592 \u001b[0m(-0.07136)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.20558 \u001b[0m(+0.00057)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.18800 \u001b[0m(-0.04561)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.20959 \u001b[0m(-0.03381)\n",
            "     | > avg_loss_0:\u001b[91m 2.91653 \u001b[0m(+0.14550)\n",
            "     | > avg_loss_gen:\u001b[92m 1.82073 \u001b[0m(-0.26804)\n",
            "     | > avg_loss_kl:\u001b[91m 7.27351 \u001b[0m(+0.26329)\n",
            "     | > avg_loss_feat:\u001b[92m 3.91237 \u001b[0m(-0.53941)\n",
            "     | > avg_loss_mel:\u001b[91m 15.98418 \u001b[0m(+0.10003)\n",
            "     | > avg_loss_duration:\u001b[92m 2.37508 \u001b[0m(-0.01611)\n",
            "     | > avg_loss_1:\u001b[92m 31.36587 \u001b[0m(-0.46024)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 328/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:31:47) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 27/49 -- GLOBAL_STEP: 1052100\u001b[0m\n",
            "     | > loss_disc: 2.46601  (2.46753)\n",
            "     | > loss_disc_real_0: 0.06714  (0.08025)\n",
            "     | > loss_disc_real_1: 0.20926  (0.22004)\n",
            "     | > loss_disc_real_2: 0.18639  (0.22713)\n",
            "     | > loss_disc_real_3: 0.20096  (0.23808)\n",
            "     | > loss_disc_real_4: 0.19428  (0.24394)\n",
            "     | > loss_disc_real_5: 0.19954  (0.24363)\n",
            "     | > loss_0: 2.46601  (2.46753)\n",
            "     | > grad_norm_0: 13.33371  (8.28849)\n",
            "     | > loss_gen: 2.30305  (2.36129)\n",
            "     | > loss_kl: 1.21685  (1.10899)\n",
            "     | > loss_feat: 5.56109  (5.01427)\n",
            "     | > loss_mel: 14.58469  (14.36342)\n",
            "     | > loss_duration: 1.03696  (1.00600)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.70264  (23.85398)\n",
            "     | > grad_norm_1: 1394.25159  (445.03220)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.53280  (0.61535)\n",
            "     | > loader_time: 0.00920  (0.00989)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.78750  (2.78750)\n",
            "     | > loss_disc_real_0: 0.49924  (0.49924)\n",
            "     | > loss_disc_real_1: 0.13243  (0.13243)\n",
            "     | > loss_disc_real_2: 0.18582  (0.18582)\n",
            "     | > loss_disc_real_3: 0.20893  (0.20893)\n",
            "     | > loss_disc_real_4: 0.18226  (0.18226)\n",
            "     | > loss_disc_real_5: 0.16288  (0.16288)\n",
            "     | > loss_0: 2.78750  (2.78750)\n",
            "     | > loss_gen: 2.03603  (2.03603)\n",
            "     | > loss_kl: 7.37330  (7.37330)\n",
            "     | > loss_feat: 5.02419  (5.02419)\n",
            "     | > loss_mel: 16.41505  (16.41505)\n",
            "     | > loss_duration: 2.28046  (2.28046)\n",
            "     | > loss_1: 33.12903  (33.12903)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.43525 \u001b[0m(+0.05215)\n",
            "     | > avg_loss_disc:\u001b[92m 2.78750 \u001b[0m(-0.12902)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.49924 \u001b[0m(+0.01485)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.13243 \u001b[0m(-0.08448)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.18582 \u001b[0m(+0.02989)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.20893 \u001b[0m(+0.00336)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.18226 \u001b[0m(-0.00574)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.16288 \u001b[0m(-0.04671)\n",
            "     | > avg_loss_0:\u001b[92m 2.78750 \u001b[0m(-0.12902)\n",
            "     | > avg_loss_gen:\u001b[91m 2.03603 \u001b[0m(+0.21529)\n",
            "     | > avg_loss_kl:\u001b[91m 7.37330 \u001b[0m(+0.09980)\n",
            "     | > avg_loss_feat:\u001b[91m 5.02419 \u001b[0m(+1.11182)\n",
            "     | > avg_loss_mel:\u001b[91m 16.41505 \u001b[0m(+0.43087)\n",
            "     | > avg_loss_duration:\u001b[92m 2.28046 \u001b[0m(-0.09462)\n",
            "     | > avg_loss_1:\u001b[91m 33.12903 \u001b[0m(+1.76316)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 329/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:32:26) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 28/49 -- GLOBAL_STEP: 1052150\u001b[0m\n",
            "     | > loss_disc: 2.48593  (2.44119)\n",
            "     | > loss_disc_real_0: 0.07259  (0.07749)\n",
            "     | > loss_disc_real_1: 0.22846  (0.22174)\n",
            "     | > loss_disc_real_2: 0.23212  (0.22808)\n",
            "     | > loss_disc_real_3: 0.23493  (0.23895)\n",
            "     | > loss_disc_real_4: 0.24544  (0.24822)\n",
            "     | > loss_disc_real_5: 0.19675  (0.23500)\n",
            "     | > loss_0: 2.48593  (2.44119)\n",
            "     | > grad_norm_0: 4.56886  (7.23238)\n",
            "     | > loss_gen: 2.17320  (2.38225)\n",
            "     | > loss_kl: 1.51670  (1.20409)\n",
            "     | > loss_feat: 5.10967  (5.16982)\n",
            "     | > loss_mel: 14.43493  (14.21271)\n",
            "     | > loss_duration: 1.09553  (1.01419)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.33004  (23.98306)\n",
            "     | > grad_norm_1: 150.23178  (253.75868)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.53170  (0.58428)\n",
            "     | > loader_time: 0.00940  (0.00885)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.59655  (2.59655)\n",
            "     | > loss_disc_real_0: 0.30671  (0.30671)\n",
            "     | > loss_disc_real_1: 0.21413  (0.21413)\n",
            "     | > loss_disc_real_2: 0.20822  (0.20822)\n",
            "     | > loss_disc_real_3: 0.20728  (0.20728)\n",
            "     | > loss_disc_real_4: 0.21356  (0.21356)\n",
            "     | > loss_disc_real_5: 0.18419  (0.18419)\n",
            "     | > loss_0: 2.59655  (2.59655)\n",
            "     | > loss_gen: 2.09528  (2.09528)\n",
            "     | > loss_kl: 7.24520  (7.24520)\n",
            "     | > loss_feat: 4.47929  (4.47929)\n",
            "     | > loss_mel: 15.17368  (15.17368)\n",
            "     | > loss_duration: 2.38787  (2.38787)\n",
            "     | > loss_1: 31.38132  (31.38132)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.36611 \u001b[0m(-0.06914)\n",
            "     | > avg_loss_disc:\u001b[92m 2.59655 \u001b[0m(-0.19095)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.30671 \u001b[0m(-0.19254)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.21413 \u001b[0m(+0.08171)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.20822 \u001b[0m(+0.02240)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.20728 \u001b[0m(-0.00166)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.21356 \u001b[0m(+0.03129)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.18419 \u001b[0m(+0.02131)\n",
            "     | > avg_loss_0:\u001b[92m 2.59655 \u001b[0m(-0.19095)\n",
            "     | > avg_loss_gen:\u001b[91m 2.09528 \u001b[0m(+0.05925)\n",
            "     | > avg_loss_kl:\u001b[92m 7.24520 \u001b[0m(-0.12810)\n",
            "     | > avg_loss_feat:\u001b[92m 4.47929 \u001b[0m(-0.54490)\n",
            "     | > avg_loss_mel:\u001b[92m 15.17368 \u001b[0m(-1.24137)\n",
            "     | > avg_loss_duration:\u001b[91m 2.38787 \u001b[0m(+0.10741)\n",
            "     | > avg_loss_1:\u001b[92m 31.38132 \u001b[0m(-1.74771)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 330/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:33:04) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 29/49 -- GLOBAL_STEP: 1052200\u001b[0m\n",
            "     | > loss_disc: 2.42113  (2.42735)\n",
            "     | > loss_disc_real_0: 0.07557  (0.06709)\n",
            "     | > loss_disc_real_1: 0.20875  (0.22249)\n",
            "     | > loss_disc_real_2: 0.22273  (0.22520)\n",
            "     | > loss_disc_real_3: 0.22865  (0.23444)\n",
            "     | > loss_disc_real_4: 0.25506  (0.24024)\n",
            "     | > loss_disc_real_5: 0.27143  (0.23372)\n",
            "     | > loss_0: 2.42113  (2.42735)\n",
            "     | > grad_norm_0: 8.18207  (6.67703)\n",
            "     | > loss_gen: 2.30225  (2.37988)\n",
            "     | > loss_kl: 1.18553  (1.23024)\n",
            "     | > loss_feat: 4.51677  (5.14022)\n",
            "     | > loss_mel: 14.71562  (14.09196)\n",
            "     | > loss_duration: 1.00479  (1.00940)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.72496  (23.85170)\n",
            "     | > grad_norm_1: 877.64044  (368.58536)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.55840  (0.59809)\n",
            "     | > loader_time: 0.00890  (0.00785)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 3.07432  (3.07432)\n",
            "     | > loss_disc_real_0: 0.84590  (0.84590)\n",
            "     | > loss_disc_real_1: 0.31273  (0.31273)\n",
            "     | > loss_disc_real_2: 0.18256  (0.18256)\n",
            "     | > loss_disc_real_3: 0.17266  (0.17266)\n",
            "     | > loss_disc_real_4: 0.18405  (0.18405)\n",
            "     | > loss_disc_real_5: 0.19601  (0.19601)\n",
            "     | > loss_0: 3.07432  (3.07432)\n",
            "     | > loss_gen: 2.33992  (2.33992)\n",
            "     | > loss_kl: 7.67531  (7.67531)\n",
            "     | > loss_feat: 5.32276  (5.32276)\n",
            "     | > loss_mel: 16.21400  (16.21400)\n",
            "     | > loss_duration: 2.33684  (2.33684)\n",
            "     | > loss_1: 33.88884  (33.88884)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.35856 \u001b[0m(-0.00755)\n",
            "     | > avg_loss_disc:\u001b[91m 3.07432 \u001b[0m(+0.47777)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.84590 \u001b[0m(+0.53919)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.31273 \u001b[0m(+0.09859)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.18256 \u001b[0m(-0.02566)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.17266 \u001b[0m(-0.03462)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.18405 \u001b[0m(-0.02951)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.19601 \u001b[0m(+0.01182)\n",
            "     | > avg_loss_0:\u001b[91m 3.07432 \u001b[0m(+0.47777)\n",
            "     | > avg_loss_gen:\u001b[91m 2.33992 \u001b[0m(+0.24465)\n",
            "     | > avg_loss_kl:\u001b[91m 7.67531 \u001b[0m(+0.43011)\n",
            "     | > avg_loss_feat:\u001b[91m 5.32276 \u001b[0m(+0.84347)\n",
            "     | > avg_loss_mel:\u001b[91m 16.21400 \u001b[0m(+1.04032)\n",
            "     | > avg_loss_duration:\u001b[92m 2.33684 \u001b[0m(-0.05104)\n",
            "     | > avg_loss_1:\u001b[91m 33.88884 \u001b[0m(+2.50751)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 331/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:33:43) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 30/49 -- GLOBAL_STEP: 1052250\u001b[0m\n",
            "     | > loss_disc: 2.46877  (2.42282)\n",
            "     | > loss_disc_real_0: 0.05262  (0.06208)\n",
            "     | > loss_disc_real_1: 0.19667  (0.21500)\n",
            "     | > loss_disc_real_2: 0.25845  (0.22568)\n",
            "     | > loss_disc_real_3: 0.22502  (0.23461)\n",
            "     | > loss_disc_real_4: 0.26381  (0.24539)\n",
            "     | > loss_disc_real_5: 0.25340  (0.23992)\n",
            "     | > loss_0: 2.46877  (2.42282)\n",
            "     | > grad_norm_0: 4.94319  (6.95912)\n",
            "     | > loss_gen: 2.35292  (2.39237)\n",
            "     | > loss_kl: 1.24576  (1.19968)\n",
            "     | > loss_feat: 4.77610  (5.19909)\n",
            "     | > loss_mel: 15.13804  (14.19152)\n",
            "     | > loss_duration: 1.05233  (1.00637)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.56516  (23.98902)\n",
            "     | > grad_norm_1: 54.10440  (367.56137)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.52620  (0.58228)\n",
            "     | > loader_time: 0.01170  (0.00728)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.65605  (2.65605)\n",
            "     | > loss_disc_real_0: 0.31081  (0.31081)\n",
            "     | > loss_disc_real_1: 0.22424  (0.22424)\n",
            "     | > loss_disc_real_2: 0.23856  (0.23856)\n",
            "     | > loss_disc_real_3: 0.22739  (0.22739)\n",
            "     | > loss_disc_real_4: 0.21435  (0.21435)\n",
            "     | > loss_disc_real_5: 0.18192  (0.18192)\n",
            "     | > loss_0: 2.65605  (2.65605)\n",
            "     | > loss_gen: 2.10397  (2.10397)\n",
            "     | > loss_kl: 7.99392  (7.99392)\n",
            "     | > loss_feat: 4.49801  (4.49801)\n",
            "     | > loss_mel: 16.27130  (16.27130)\n",
            "     | > loss_duration: 2.31453  (2.31453)\n",
            "     | > loss_1: 33.18173  (33.18173)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.37018 \u001b[0m(+0.01162)\n",
            "     | > avg_loss_disc:\u001b[92m 2.65605 \u001b[0m(-0.41828)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.31081 \u001b[0m(-0.53509)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.22424 \u001b[0m(-0.08848)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.23856 \u001b[0m(+0.05600)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.22739 \u001b[0m(+0.05473)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.21435 \u001b[0m(+0.03030)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.18192 \u001b[0m(-0.01409)\n",
            "     | > avg_loss_0:\u001b[92m 2.65605 \u001b[0m(-0.41828)\n",
            "     | > avg_loss_gen:\u001b[92m 2.10397 \u001b[0m(-0.23596)\n",
            "     | > avg_loss_kl:\u001b[91m 7.99392 \u001b[0m(+0.31860)\n",
            "     | > avg_loss_feat:\u001b[92m 4.49801 \u001b[0m(-0.82475)\n",
            "     | > avg_loss_mel:\u001b[91m 16.27130 \u001b[0m(+0.05729)\n",
            "     | > avg_loss_duration:\u001b[92m 2.31453 \u001b[0m(-0.02230)\n",
            "     | > avg_loss_1:\u001b[92m 33.18173 \u001b[0m(-0.70711)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 332/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:34:20) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 31/49 -- GLOBAL_STEP: 1052300\u001b[0m\n",
            "     | > loss_disc: 2.55637  (2.44864)\n",
            "     | > loss_disc_real_0: 0.07707  (0.09589)\n",
            "     | > loss_disc_real_1: 0.19596  (0.21933)\n",
            "     | > loss_disc_real_2: 0.18274  (0.22381)\n",
            "     | > loss_disc_real_3: 0.21347  (0.23085)\n",
            "     | > loss_disc_real_4: 0.24032  (0.23996)\n",
            "     | > loss_disc_real_5: 0.27275  (0.24054)\n",
            "     | > loss_0: 2.55637  (2.44864)\n",
            "     | > grad_norm_0: 7.23659  (9.24848)\n",
            "     | > loss_gen: 2.02142  (2.37451)\n",
            "     | > loss_kl: 1.75397  (1.14974)\n",
            "     | > loss_feat: 4.90249  (5.17681)\n",
            "     | > loss_mel: 14.59471  (14.32300)\n",
            "     | > loss_duration: 1.04374  (1.01096)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.31633  (24.03501)\n",
            "     | > grad_norm_1: 699.47443  (503.56644)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.53060  (0.60265)\n",
            "     | > loader_time: 0.00590  (0.00780)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.79591  (2.79591)\n",
            "     | > loss_disc_real_0: 0.44026  (0.44026)\n",
            "     | > loss_disc_real_1: 0.25484  (0.25484)\n",
            "     | > loss_disc_real_2: 0.24068  (0.24068)\n",
            "     | > loss_disc_real_3: 0.15202  (0.15202)\n",
            "     | > loss_disc_real_4: 0.19534  (0.19534)\n",
            "     | > loss_disc_real_5: 0.14472  (0.14472)\n",
            "     | > loss_0: 2.79591  (2.79591)\n",
            "     | > loss_gen: 2.07632  (2.07632)\n",
            "     | > loss_kl: 7.85457  (7.85457)\n",
            "     | > loss_feat: 4.74847  (4.74847)\n",
            "     | > loss_mel: 15.93191  (15.93191)\n",
            "     | > loss_duration: 2.42162  (2.42162)\n",
            "     | > loss_1: 33.03289  (33.03289)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.51704 \u001b[0m(+0.14685)\n",
            "     | > avg_loss_disc:\u001b[91m 2.79591 \u001b[0m(+0.13987)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.44026 \u001b[0m(+0.12945)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.25484 \u001b[0m(+0.03060)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.24068 \u001b[0m(+0.00212)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.15202 \u001b[0m(-0.07538)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.19534 \u001b[0m(-0.01901)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.14472 \u001b[0m(-0.03720)\n",
            "     | > avg_loss_0:\u001b[91m 2.79591 \u001b[0m(+0.13987)\n",
            "     | > avg_loss_gen:\u001b[92m 2.07632 \u001b[0m(-0.02765)\n",
            "     | > avg_loss_kl:\u001b[92m 7.85457 \u001b[0m(-0.13935)\n",
            "     | > avg_loss_feat:\u001b[91m 4.74847 \u001b[0m(+0.25046)\n",
            "     | > avg_loss_mel:\u001b[92m 15.93191 \u001b[0m(-0.33939)\n",
            "     | > avg_loss_duration:\u001b[91m 2.42162 \u001b[0m(+0.10709)\n",
            "     | > avg_loss_1:\u001b[92m 33.03289 \u001b[0m(-0.14883)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 333/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:34:59) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 32/49 -- GLOBAL_STEP: 1052350\u001b[0m\n",
            "     | > loss_disc: 2.40987  (2.42576)\n",
            "     | > loss_disc_real_0: 0.03134  (0.07017)\n",
            "     | > loss_disc_real_1: 0.20502  (0.21794)\n",
            "     | > loss_disc_real_2: 0.21277  (0.22159)\n",
            "     | > loss_disc_real_3: 0.25345  (0.23164)\n",
            "     | > loss_disc_real_4: 0.25051  (0.24372)\n",
            "     | > loss_disc_real_5: 0.24684  (0.23930)\n",
            "     | > loss_0: 2.40987  (2.42576)\n",
            "     | > grad_norm_0: 6.20980  (6.70483)\n",
            "     | > loss_gen: 2.24041  (2.35803)\n",
            "     | > loss_kl: 1.70919  (1.17949)\n",
            "     | > loss_feat: 5.18695  (5.17523)\n",
            "     | > loss_mel: 14.91519  (14.35841)\n",
            "     | > loss_duration: 1.09757  (1.00650)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 25.14931  (24.07766)\n",
            "     | > grad_norm_1: 584.02466  (333.94470)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.71470  (0.61181)\n",
            "     | > loader_time: 0.00470  (0.00910)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.85852  (2.85852)\n",
            "     | > loss_disc_real_0: 0.44031  (0.44031)\n",
            "     | > loss_disc_real_1: 0.22141  (0.22141)\n",
            "     | > loss_disc_real_2: 0.19967  (0.19967)\n",
            "     | > loss_disc_real_3: 0.23716  (0.23716)\n",
            "     | > loss_disc_real_4: 0.24988  (0.24988)\n",
            "     | > loss_disc_real_5: 0.20466  (0.20466)\n",
            "     | > loss_0: 2.85852  (2.85852)\n",
            "     | > loss_gen: 2.06480  (2.06480)\n",
            "     | > loss_kl: 7.33611  (7.33611)\n",
            "     | > loss_feat: 4.43187  (4.43187)\n",
            "     | > loss_mel: 15.62507  (15.62507)\n",
            "     | > loss_duration: 2.38977  (2.38977)\n",
            "     | > loss_1: 31.84761  (31.84761)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.35891 \u001b[0m(-0.15813)\n",
            "     | > avg_loss_disc:\u001b[91m 2.85852 \u001b[0m(+0.06261)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.44031 \u001b[0m(+0.00005)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.22141 \u001b[0m(-0.03343)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.19967 \u001b[0m(-0.04101)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.23716 \u001b[0m(+0.08515)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.24988 \u001b[0m(+0.05454)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.20466 \u001b[0m(+0.05993)\n",
            "     | > avg_loss_0:\u001b[91m 2.85852 \u001b[0m(+0.06261)\n",
            "     | > avg_loss_gen:\u001b[92m 2.06480 \u001b[0m(-0.01152)\n",
            "     | > avg_loss_kl:\u001b[92m 7.33611 \u001b[0m(-0.51846)\n",
            "     | > avg_loss_feat:\u001b[92m 4.43187 \u001b[0m(-0.31660)\n",
            "     | > avg_loss_mel:\u001b[92m 15.62507 \u001b[0m(-0.30684)\n",
            "     | > avg_loss_duration:\u001b[92m 2.38977 \u001b[0m(-0.03186)\n",
            "     | > avg_loss_1:\u001b[92m 31.84761 \u001b[0m(-1.18528)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 334/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:35:38) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 33/49 -- GLOBAL_STEP: 1052400\u001b[0m\n",
            "     | > loss_disc: 2.40108  (2.44077)\n",
            "     | > loss_disc_real_0: 0.04938  (0.07425)\n",
            "     | > loss_disc_real_1: 0.25242  (0.21959)\n",
            "     | > loss_disc_real_2: 0.21932  (0.22149)\n",
            "     | > loss_disc_real_3: 0.22868  (0.23480)\n",
            "     | > loss_disc_real_4: 0.21591  (0.24168)\n",
            "     | > loss_disc_real_5: 0.23948  (0.23602)\n",
            "     | > loss_0: 2.40108  (2.44077)\n",
            "     | > grad_norm_0: 4.44331  (5.81997)\n",
            "     | > loss_gen: 2.37969  (2.34843)\n",
            "     | > loss_kl: 1.24956  (1.05812)\n",
            "     | > loss_feat: 5.22431  (5.08399)\n",
            "     | > loss_mel: 14.58926  (14.31502)\n",
            "     | > loss_duration: 1.07331  (1.01278)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.51613  (23.81834)\n",
            "     | > grad_norm_1: 348.79929  (176.41423)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.55720  (0.61548)\n",
            "     | > loader_time: 0.01260  (0.00834)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 3.01789  (3.01789)\n",
            "     | > loss_disc_real_0: 0.54972  (0.54972)\n",
            "     | > loss_disc_real_1: 0.21502  (0.21502)\n",
            "     | > loss_disc_real_2: 0.21085  (0.21085)\n",
            "     | > loss_disc_real_3: 0.20276  (0.20276)\n",
            "     | > loss_disc_real_4: 0.22255  (0.22255)\n",
            "     | > loss_disc_real_5: 0.31286  (0.31286)\n",
            "     | > loss_0: 3.01789  (3.01789)\n",
            "     | > loss_gen: 2.05558  (2.05558)\n",
            "     | > loss_kl: 7.35923  (7.35923)\n",
            "     | > loss_feat: 4.54616  (4.54616)\n",
            "     | > loss_mel: 16.12857  (16.12857)\n",
            "     | > loss_duration: 2.36197  (2.36197)\n",
            "     | > loss_1: 32.45152  (32.45152)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.36964 \u001b[0m(+0.01073)\n",
            "     | > avg_loss_disc:\u001b[91m 3.01789 \u001b[0m(+0.15936)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.54972 \u001b[0m(+0.10942)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.21502 \u001b[0m(-0.00639)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.21085 \u001b[0m(+0.01118)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.20276 \u001b[0m(-0.03441)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.22255 \u001b[0m(-0.02733)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.31286 \u001b[0m(+0.10821)\n",
            "     | > avg_loss_0:\u001b[91m 3.01789 \u001b[0m(+0.15936)\n",
            "     | > avg_loss_gen:\u001b[92m 2.05558 \u001b[0m(-0.00922)\n",
            "     | > avg_loss_kl:\u001b[91m 7.35923 \u001b[0m(+0.02313)\n",
            "     | > avg_loss_feat:\u001b[91m 4.54616 \u001b[0m(+0.11429)\n",
            "     | > avg_loss_mel:\u001b[91m 16.12857 \u001b[0m(+0.50350)\n",
            "     | > avg_loss_duration:\u001b[92m 2.36197 \u001b[0m(-0.02779)\n",
            "     | > avg_loss_1:\u001b[91m 32.45152 \u001b[0m(+0.60390)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 335/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:36:16) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 34/49 -- GLOBAL_STEP: 1052450\u001b[0m\n",
            "     | > loss_disc: 2.52638  (2.38563)\n",
            "     | > loss_disc_real_0: 0.07613  (0.06620)\n",
            "     | > loss_disc_real_1: 0.16833  (0.21282)\n",
            "     | > loss_disc_real_2: 0.20186  (0.22316)\n",
            "     | > loss_disc_real_3: 0.28569  (0.22856)\n",
            "     | > loss_disc_real_4: 0.25690  (0.24290)\n",
            "     | > loss_disc_real_5: 0.24031  (0.23040)\n",
            "     | > loss_0: 2.52638  (2.38563)\n",
            "     | > grad_norm_0: 5.56423  (7.14652)\n",
            "     | > loss_gen: 2.20463  (2.41141)\n",
            "     | > loss_kl: 1.00836  (1.16232)\n",
            "     | > loss_feat: 4.58906  (5.34729)\n",
            "     | > loss_mel: 14.74921  (14.38282)\n",
            "     | > loss_duration: 1.03848  (1.02912)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.58974  (24.33297)\n",
            "     | > grad_norm_1: 140.01642  (432.62167)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.53700  (0.61330)\n",
            "     | > loader_time: 0.01150  (0.00891)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.66436  (2.66436)\n",
            "     | > loss_disc_real_0: 0.37588  (0.37588)\n",
            "     | > loss_disc_real_1: 0.18076  (0.18076)\n",
            "     | > loss_disc_real_2: 0.22095  (0.22095)\n",
            "     | > loss_disc_real_3: 0.17038  (0.17038)\n",
            "     | > loss_disc_real_4: 0.18218  (0.18218)\n",
            "     | > loss_disc_real_5: 0.26018  (0.26018)\n",
            "     | > loss_0: 2.66436  (2.66436)\n",
            "     | > loss_gen: 2.24249  (2.24249)\n",
            "     | > loss_kl: 7.12562  (7.12562)\n",
            "     | > loss_feat: 4.45345  (4.45345)\n",
            "     | > loss_mel: 15.08143  (15.08143)\n",
            "     | > loss_duration: 2.34033  (2.34033)\n",
            "     | > loss_1: 31.24332  (31.24332)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.37764 \u001b[0m(+0.00801)\n",
            "     | > avg_loss_disc:\u001b[92m 2.66436 \u001b[0m(-0.35352)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.37588 \u001b[0m(-0.17385)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.18076 \u001b[0m(-0.03426)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.22095 \u001b[0m(+0.01011)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.17038 \u001b[0m(-0.03238)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.18218 \u001b[0m(-0.04036)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.26018 \u001b[0m(-0.05268)\n",
            "     | > avg_loss_0:\u001b[92m 2.66436 \u001b[0m(-0.35352)\n",
            "     | > avg_loss_gen:\u001b[91m 2.24249 \u001b[0m(+0.18690)\n",
            "     | > avg_loss_kl:\u001b[92m 7.12562 \u001b[0m(-0.23361)\n",
            "     | > avg_loss_feat:\u001b[92m 4.45345 \u001b[0m(-0.09271)\n",
            "     | > avg_loss_mel:\u001b[92m 15.08143 \u001b[0m(-1.04714)\n",
            "     | > avg_loss_duration:\u001b[92m 2.34033 \u001b[0m(-0.02164)\n",
            "     | > avg_loss_1:\u001b[92m 31.24332 \u001b[0m(-1.20820)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 336/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:36:55) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 35/49 -- GLOBAL_STEP: 1052500\u001b[0m\n",
            "     | > loss_disc: 2.38383  (2.40226)\n",
            "     | > loss_disc_real_0: 0.08109  (0.07350)\n",
            "     | > loss_disc_real_1: 0.21300  (0.21561)\n",
            "     | > loss_disc_real_2: 0.21039  (0.22496)\n",
            "     | > loss_disc_real_3: 0.19857  (0.22984)\n",
            "     | > loss_disc_real_4: 0.26374  (0.24379)\n",
            "     | > loss_disc_real_5: 0.18649  (0.23629)\n",
            "     | > loss_0: 2.38383  (2.40226)\n",
            "     | > grad_norm_0: 4.25209  (6.10980)\n",
            "     | > loss_gen: 2.25198  (2.39324)\n",
            "     | > loss_kl: 1.28425  (1.13793)\n",
            "     | > loss_feat: 4.71242  (5.21048)\n",
            "     | > loss_mel: 14.82349  (14.20020)\n",
            "     | > loss_duration: 1.06233  (1.02817)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.13446  (23.97002)\n",
            "     | > grad_norm_1: 145.26265  (206.49280)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.55150  (0.59189)\n",
            "     | > loader_time: 0.01210  (0.00764)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.86986  (2.86986)\n",
            "     | > loss_disc_real_0: 0.35564  (0.35564)\n",
            "     | > loss_disc_real_1: 0.18852  (0.18852)\n",
            "     | > loss_disc_real_2: 0.20454  (0.20454)\n",
            "     | > loss_disc_real_3: 0.25487  (0.25487)\n",
            "     | > loss_disc_real_4: 0.29038  (0.29038)\n",
            "     | > loss_disc_real_5: 0.17665  (0.17665)\n",
            "     | > loss_0: 2.86986  (2.86986)\n",
            "     | > loss_gen: 1.94046  (1.94046)\n",
            "     | > loss_kl: 7.59749  (7.59749)\n",
            "     | > loss_feat: 4.79677  (4.79677)\n",
            "     | > loss_mel: 15.55808  (15.55808)\n",
            "     | > loss_duration: 2.28596  (2.28596)\n",
            "     | > loss_1: 32.17876  (32.17876)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.35947 \u001b[0m(-0.01817)\n",
            "     | > avg_loss_disc:\u001b[91m 2.86986 \u001b[0m(+0.20550)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.35564 \u001b[0m(-0.02023)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.18852 \u001b[0m(+0.00776)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.20454 \u001b[0m(-0.01642)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.25487 \u001b[0m(+0.08449)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.29038 \u001b[0m(+0.10820)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.17665 \u001b[0m(-0.08353)\n",
            "     | > avg_loss_0:\u001b[91m 2.86986 \u001b[0m(+0.20550)\n",
            "     | > avg_loss_gen:\u001b[92m 1.94046 \u001b[0m(-0.30202)\n",
            "     | > avg_loss_kl:\u001b[91m 7.59749 \u001b[0m(+0.47187)\n",
            "     | > avg_loss_feat:\u001b[91m 4.79677 \u001b[0m(+0.34332)\n",
            "     | > avg_loss_mel:\u001b[91m 15.55808 \u001b[0m(+0.47665)\n",
            "     | > avg_loss_duration:\u001b[92m 2.28596 \u001b[0m(-0.05438)\n",
            "     | > avg_loss_1:\u001b[91m 32.17876 \u001b[0m(+0.93545)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 337/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:37:32) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 36/49 -- GLOBAL_STEP: 1052550\u001b[0m\n",
            "     | > loss_disc: 2.38918  (2.41549)\n",
            "     | > loss_disc_real_0: 0.11007  (0.07278)\n",
            "     | > loss_disc_real_1: 0.23689  (0.22045)\n",
            "     | > loss_disc_real_2: 0.22326  (0.22849)\n",
            "     | > loss_disc_real_3: 0.23986  (0.23500)\n",
            "     | > loss_disc_real_4: 0.28191  (0.24490)\n",
            "     | > loss_disc_real_5: 0.28139  (0.23156)\n",
            "     | > loss_0: 2.38918  (2.41549)\n",
            "     | > grad_norm_0: 6.48716  (6.98160)\n",
            "     | > loss_gen: 2.41149  (2.38996)\n",
            "     | > loss_kl: 1.03870  (1.14150)\n",
            "     | > loss_feat: 5.04418  (5.24609)\n",
            "     | > loss_mel: 14.51858  (14.34681)\n",
            "     | > loss_duration: 1.06535  (1.01700)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.07830  (24.14137)\n",
            "     | > grad_norm_1: 46.54356  (285.10800)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.56770  (0.62271)\n",
            "     | > loader_time: 0.01220  (0.00933)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 3.08025  (3.08025)\n",
            "     | > loss_disc_real_0: 0.65449  (0.65449)\n",
            "     | > loss_disc_real_1: 0.21404  (0.21404)\n",
            "     | > loss_disc_real_2: 0.23304  (0.23304)\n",
            "     | > loss_disc_real_3: 0.23034  (0.23034)\n",
            "     | > loss_disc_real_4: 0.23970  (0.23970)\n",
            "     | > loss_disc_real_5: 0.15608  (0.15608)\n",
            "     | > loss_0: 3.08025  (3.08025)\n",
            "     | > loss_gen: 2.03090  (2.03090)\n",
            "     | > loss_kl: 7.70463  (7.70463)\n",
            "     | > loss_feat: 3.96714  (3.96714)\n",
            "     | > loss_mel: 15.69982  (15.69982)\n",
            "     | > loss_duration: 2.36099  (2.36099)\n",
            "     | > loss_1: 31.76348  (31.76348)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.47998 \u001b[0m(+0.12050)\n",
            "     | > avg_loss_disc:\u001b[91m 3.08025 \u001b[0m(+0.21039)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.65449 \u001b[0m(+0.29885)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.21404 \u001b[0m(+0.02552)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.23304 \u001b[0m(+0.02850)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.23034 \u001b[0m(-0.02453)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.23970 \u001b[0m(-0.05068)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.15608 \u001b[0m(-0.02057)\n",
            "     | > avg_loss_0:\u001b[91m 3.08025 \u001b[0m(+0.21039)\n",
            "     | > avg_loss_gen:\u001b[91m 2.03090 \u001b[0m(+0.09044)\n",
            "     | > avg_loss_kl:\u001b[91m 7.70463 \u001b[0m(+0.10714)\n",
            "     | > avg_loss_feat:\u001b[92m 3.96714 \u001b[0m(-0.82963)\n",
            "     | > avg_loss_mel:\u001b[91m 15.69982 \u001b[0m(+0.14174)\n",
            "     | > avg_loss_duration:\u001b[91m 2.36099 \u001b[0m(+0.07503)\n",
            "     | > avg_loss_1:\u001b[92m 31.76348 \u001b[0m(-0.41529)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 338/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:38:11) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 37/49 -- GLOBAL_STEP: 1052600\u001b[0m\n",
            "     | > loss_disc: 2.53682  (2.42808)\n",
            "     | > loss_disc_real_0: 0.07251  (0.06900)\n",
            "     | > loss_disc_real_1: 0.24932  (0.22174)\n",
            "     | > loss_disc_real_2: 0.27591  (0.22536)\n",
            "     | > loss_disc_real_3: 0.26528  (0.23598)\n",
            "     | > loss_disc_real_4: 0.28488  (0.24074)\n",
            "     | > loss_disc_real_5: 0.27976  (0.23939)\n",
            "     | > loss_0: 2.53682  (2.42808)\n",
            "     | > grad_norm_0: 6.51905  (5.70774)\n",
            "     | > loss_gen: 2.28355  (2.36412)\n",
            "     | > loss_kl: 1.10030  (1.14683)\n",
            "     | > loss_feat: 4.53566  (5.21421)\n",
            "     | > loss_mel: 14.60610  (14.33839)\n",
            "     | > loss_duration: 1.10652  (1.02631)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 23.63213  (24.08985)\n",
            "     | > grad_norm_1: 200.19150  (279.14426)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.62640  (0.58596)\n",
            "     | > loader_time: 0.01040  (0.00932)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.92177  (2.92177)\n",
            "     | > loss_disc_real_0: 0.41784  (0.41784)\n",
            "     | > loss_disc_real_1: 0.25845  (0.25845)\n",
            "     | > loss_disc_real_2: 0.21176  (0.21176)\n",
            "     | > loss_disc_real_3: 0.19239  (0.19239)\n",
            "     | > loss_disc_real_4: 0.18483  (0.18483)\n",
            "     | > loss_disc_real_5: 0.29915  (0.29915)\n",
            "     | > loss_0: 2.92177  (2.92177)\n",
            "     | > loss_gen: 2.12971  (2.12971)\n",
            "     | > loss_kl: 6.94636  (6.94636)\n",
            "     | > loss_feat: 3.68678  (3.68678)\n",
            "     | > loss_mel: 16.41249  (16.41249)\n",
            "     | > loss_duration: 2.40468  (2.40468)\n",
            "     | > loss_1: 31.58002  (31.58002)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.36518 \u001b[0m(-0.11480)\n",
            "     | > avg_loss_disc:\u001b[92m 2.92177 \u001b[0m(-0.15848)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.41784 \u001b[0m(-0.23665)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.25845 \u001b[0m(+0.04441)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.21176 \u001b[0m(-0.02128)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.19239 \u001b[0m(-0.03795)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.18483 \u001b[0m(-0.05487)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.29915 \u001b[0m(+0.14307)\n",
            "     | > avg_loss_0:\u001b[92m 2.92177 \u001b[0m(-0.15848)\n",
            "     | > avg_loss_gen:\u001b[91m 2.12971 \u001b[0m(+0.09880)\n",
            "     | > avg_loss_kl:\u001b[92m 6.94636 \u001b[0m(-0.75827)\n",
            "     | > avg_loss_feat:\u001b[92m 3.68678 \u001b[0m(-0.28036)\n",
            "     | > avg_loss_mel:\u001b[91m 16.41249 \u001b[0m(+0.71268)\n",
            "     | > avg_loss_duration:\u001b[91m 2.40468 \u001b[0m(+0.04369)\n",
            "     | > avg_loss_1:\u001b[92m 31.58002 \u001b[0m(-0.18345)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 339/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:38:47) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 38/49 -- GLOBAL_STEP: 1052650\u001b[0m\n",
            "     | > loss_disc: 2.55316  (2.42573)\n",
            "     | > loss_disc_real_0: 0.06990  (0.07116)\n",
            "     | > loss_disc_real_1: 0.19548  (0.21749)\n",
            "     | > loss_disc_real_2: 0.23377  (0.23049)\n",
            "     | > loss_disc_real_3: 0.23523  (0.23445)\n",
            "     | > loss_disc_real_4: 0.24048  (0.24401)\n",
            "     | > loss_disc_real_5: 0.21868  (0.22956)\n",
            "     | > loss_0: 2.55316  (2.42573)\n",
            "     | > grad_norm_0: 5.93342  (6.47820)\n",
            "     | > loss_gen: 2.16085  (2.36030)\n",
            "     | > loss_kl: 1.40724  (1.17982)\n",
            "     | > loss_feat: 4.82777  (5.16126)\n",
            "     | > loss_mel: 15.46250  (14.41974)\n",
            "     | > loss_duration: 1.11744  (1.02371)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.97579  (24.14483)\n",
            "     | > grad_norm_1: 74.51871  (325.22934)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.59840  (0.60422)\n",
            "     | > loader_time: 0.00530  (0.00838)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.73758  (2.73758)\n",
            "     | > loss_disc_real_0: 0.34996  (0.34996)\n",
            "     | > loss_disc_real_1: 0.19559  (0.19559)\n",
            "     | > loss_disc_real_2: 0.17677  (0.17677)\n",
            "     | > loss_disc_real_3: 0.29427  (0.29427)\n",
            "     | > loss_disc_real_4: 0.28215  (0.28215)\n",
            "     | > loss_disc_real_5: 0.15539  (0.15539)\n",
            "     | > loss_0: 2.73758  (2.73758)\n",
            "     | > loss_gen: 2.33171  (2.33171)\n",
            "     | > loss_kl: 7.42263  (7.42263)\n",
            "     | > loss_feat: 4.16545  (4.16545)\n",
            "     | > loss_mel: 15.79410  (15.79410)\n",
            "     | > loss_duration: 2.34596  (2.34596)\n",
            "     | > loss_1: 32.05985  (32.05985)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.38562 \u001b[0m(+0.02044)\n",
            "     | > avg_loss_disc:\u001b[92m 2.73758 \u001b[0m(-0.18419)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.34996 \u001b[0m(-0.06788)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.19559 \u001b[0m(-0.06287)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.17677 \u001b[0m(-0.03499)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.29427 \u001b[0m(+0.10187)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.28215 \u001b[0m(+0.09732)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.15539 \u001b[0m(-0.14376)\n",
            "     | > avg_loss_0:\u001b[92m 2.73758 \u001b[0m(-0.18419)\n",
            "     | > avg_loss_gen:\u001b[91m 2.33171 \u001b[0m(+0.20201)\n",
            "     | > avg_loss_kl:\u001b[91m 7.42263 \u001b[0m(+0.47627)\n",
            "     | > avg_loss_feat:\u001b[91m 4.16545 \u001b[0m(+0.47867)\n",
            "     | > avg_loss_mel:\u001b[92m 15.79410 \u001b[0m(-0.61839)\n",
            "     | > avg_loss_duration:\u001b[92m 2.34596 \u001b[0m(-0.05872)\n",
            "     | > avg_loss_1:\u001b[91m 32.05985 \u001b[0m(+0.47983)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 340/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:39:26) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 39/49 -- GLOBAL_STEP: 1052700\u001b[0m\n",
            "     | > loss_disc: 2.36516  (2.41284)\n",
            "     | > loss_disc_real_0: 0.05130  (0.06397)\n",
            "     | > loss_disc_real_1: 0.21202  (0.22372)\n",
            "     | > loss_disc_real_2: 0.22859  (0.22334)\n",
            "     | > loss_disc_real_3: 0.20837  (0.23557)\n",
            "     | > loss_disc_real_4: 0.23268  (0.24098)\n",
            "     | > loss_disc_real_5: 0.17674  (0.23030)\n",
            "     | > loss_0: 2.36516  (2.41284)\n",
            "     | > grad_norm_0: 3.26368  (6.17783)\n",
            "     | > loss_gen: 2.29102  (2.39152)\n",
            "     | > loss_kl: 1.22885  (1.17180)\n",
            "     | > loss_feat: 5.33799  (5.29889)\n",
            "     | > loss_mel: 14.66955  (14.25969)\n",
            "     | > loss_duration: 1.11213  (1.02075)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.63954  (24.14265)\n",
            "     | > grad_norm_1: 327.64926  (323.73364)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.71040  (0.59078)\n",
            "     | > loader_time: 0.00520  (0.00888)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.73736  (2.73736)\n",
            "     | > loss_disc_real_0: 0.34362  (0.34362)\n",
            "     | > loss_disc_real_1: 0.17654  (0.17654)\n",
            "     | > loss_disc_real_2: 0.23901  (0.23901)\n",
            "     | > loss_disc_real_3: 0.20938  (0.20938)\n",
            "     | > loss_disc_real_4: 0.23809  (0.23809)\n",
            "     | > loss_disc_real_5: 0.14762  (0.14762)\n",
            "     | > loss_0: 2.73736  (2.73736)\n",
            "     | > loss_gen: 1.95594  (1.95594)\n",
            "     | > loss_kl: 7.26384  (7.26384)\n",
            "     | > loss_feat: 4.90270  (4.90270)\n",
            "     | > loss_mel: 15.59720  (15.59720)\n",
            "     | > loss_duration: 2.31917  (2.31917)\n",
            "     | > loss_1: 32.03885  (32.03885)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.36180 \u001b[0m(-0.02382)\n",
            "     | > avg_loss_disc:\u001b[92m 2.73736 \u001b[0m(-0.00022)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.34362 \u001b[0m(-0.00634)\n",
            "     | > avg_loss_disc_real_1:\u001b[92m 0.17654 \u001b[0m(-0.01905)\n",
            "     | > avg_loss_disc_real_2:\u001b[91m 0.23901 \u001b[0m(+0.06224)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.20938 \u001b[0m(-0.08488)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.23809 \u001b[0m(-0.04406)\n",
            "     | > avg_loss_disc_real_5:\u001b[92m 0.14762 \u001b[0m(-0.00777)\n",
            "     | > avg_loss_0:\u001b[92m 2.73736 \u001b[0m(-0.00022)\n",
            "     | > avg_loss_gen:\u001b[92m 1.95594 \u001b[0m(-0.37578)\n",
            "     | > avg_loss_kl:\u001b[92m 7.26384 \u001b[0m(-0.15879)\n",
            "     | > avg_loss_feat:\u001b[91m 4.90270 \u001b[0m(+0.73725)\n",
            "     | > avg_loss_mel:\u001b[92m 15.59720 \u001b[0m(-0.19691)\n",
            "     | > avg_loss_duration:\u001b[92m 2.31917 \u001b[0m(-0.02679)\n",
            "     | > avg_loss_1:\u001b[92m 32.03885 \u001b[0m(-0.02101)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 341/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:40:03) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 40/49 -- GLOBAL_STEP: 1052750\u001b[0m\n",
            "     | > loss_disc: 2.61897  (2.43458)\n",
            "     | > loss_disc_real_0: 0.09207  (0.06566)\n",
            "     | > loss_disc_real_1: 0.28639  (0.22096)\n",
            "     | > loss_disc_real_2: 0.24102  (0.22076)\n",
            "     | > loss_disc_real_3: 0.27408  (0.23216)\n",
            "     | > loss_disc_real_4: 0.31205  (0.24560)\n",
            "     | > loss_disc_real_5: 0.32513  (0.24333)\n",
            "     | > loss_0: 2.61897  (2.43458)\n",
            "     | > grad_norm_0: 4.99585  (6.41859)\n",
            "     | > loss_gen: 2.46860  (2.39019)\n",
            "     | > loss_kl: 1.60891  (1.23009)\n",
            "     | > loss_feat: 4.25325  (5.28636)\n",
            "     | > loss_mel: 14.72069  (14.39725)\n",
            "     | > loss_duration: 1.09840  (1.02460)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.14985  (24.32849)\n",
            "     | > grad_norm_1: 404.12048  (354.05112)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.58120  (0.60949)\n",
            "     | > loader_time: 0.00850  (0.00891)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 3.28704  (3.28704)\n",
            "     | > loss_disc_real_0: 0.72972  (0.72972)\n",
            "     | > loss_disc_real_1: 0.18502  (0.18502)\n",
            "     | > loss_disc_real_2: 0.23631  (0.23631)\n",
            "     | > loss_disc_real_3: 0.26777  (0.26777)\n",
            "     | > loss_disc_real_4: 0.22615  (0.22615)\n",
            "     | > loss_disc_real_5: 0.21207  (0.21207)\n",
            "     | > loss_0: 3.28704  (3.28704)\n",
            "     | > loss_gen: 2.10609  (2.10609)\n",
            "     | > loss_kl: 7.46833  (7.46833)\n",
            "     | > loss_feat: 4.21300  (4.21300)\n",
            "     | > loss_mel: 15.47564  (15.47564)\n",
            "     | > loss_duration: 2.47177  (2.47177)\n",
            "     | > loss_1: 31.73483  (31.73483)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.52654 \u001b[0m(+0.16474)\n",
            "     | > avg_loss_disc:\u001b[91m 3.28704 \u001b[0m(+0.54969)\n",
            "     | > avg_loss_disc_real_0:\u001b[91m 0.72972 \u001b[0m(+0.38610)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.18502 \u001b[0m(+0.00849)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.23631 \u001b[0m(-0.00271)\n",
            "     | > avg_loss_disc_real_3:\u001b[91m 0.26777 \u001b[0m(+0.05839)\n",
            "     | > avg_loss_disc_real_4:\u001b[92m 0.22615 \u001b[0m(-0.01194)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.21207 \u001b[0m(+0.06445)\n",
            "     | > avg_loss_0:\u001b[91m 3.28704 \u001b[0m(+0.54969)\n",
            "     | > avg_loss_gen:\u001b[91m 2.10609 \u001b[0m(+0.15015)\n",
            "     | > avg_loss_kl:\u001b[91m 7.46833 \u001b[0m(+0.20450)\n",
            "     | > avg_loss_feat:\u001b[92m 4.21300 \u001b[0m(-0.68970)\n",
            "     | > avg_loss_mel:\u001b[92m 15.47564 \u001b[0m(-0.12155)\n",
            "     | > avg_loss_duration:\u001b[91m 2.47177 \u001b[0m(+0.15259)\n",
            "     | > avg_loss_1:\u001b[92m 31.73483 \u001b[0m(-0.30401)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 342/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:40:41) \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 41/49 -- GLOBAL_STEP: 1052800\u001b[0m\n",
            "     | > loss_disc: 2.48604  (2.41994)\n",
            "     | > loss_disc_real_0: 0.07316  (0.06957)\n",
            "     | > loss_disc_real_1: 0.20424  (0.21732)\n",
            "     | > loss_disc_real_2: 0.25894  (0.22152)\n",
            "     | > loss_disc_real_3: 0.25082  (0.23506)\n",
            "     | > loss_disc_real_4: 0.27342  (0.24140)\n",
            "     | > loss_disc_real_5: 0.21180  (0.23469)\n",
            "     | > loss_0: 2.48604  (2.41994)\n",
            "     | > grad_norm_0: 3.91630  (5.96773)\n",
            "     | > loss_gen: 2.30358  (2.35873)\n",
            "     | > loss_kl: 1.42732  (1.22814)\n",
            "     | > loss_feat: 4.63986  (5.17450)\n",
            "     | > loss_mel: 14.66993  (14.35942)\n",
            "     | > loss_duration: 1.12552  (1.03089)\n",
            "     | > amp_scaler: 128.00000  (128.00000)\n",
            "     | > loss_1: 24.16622  (24.15167)\n",
            "     | > grad_norm_1: 183.57701  (309.14130)\n",
            "     | > current_lr_0: 0.00019 \n",
            "     | > current_lr_1: 0.00019 \n",
            "     | > step_time: 0.71610  (0.61393)\n",
            "     | > loader_time: 0.00560  (0.00864)\n",
            "\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 3\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 114\n",
            " | > Min text length: 50\n",
            " | > Avg text length: 78.0\n",
            " | \n",
            " | > Max audio length: 156819.0\n",
            " | > Min audio length: 69606.0\n",
            " | > Avg audio length: 104756.66666666667\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n",
            "\u001b[1m   --> STEP: 0\u001b[0m\n",
            "     | > loss_disc: 2.80673  (2.80673)\n",
            "     | > loss_disc_real_0: 0.57194  (0.57194)\n",
            "     | > loss_disc_real_1: 0.18746  (0.18746)\n",
            "     | > loss_disc_real_2: 0.17755  (0.17755)\n",
            "     | > loss_disc_real_3: 0.24213  (0.24213)\n",
            "     | > loss_disc_real_4: 0.23505  (0.23505)\n",
            "     | > loss_disc_real_5: 0.21293  (0.21293)\n",
            "     | > loss_0: 2.80673  (2.80673)\n",
            "     | > loss_gen: 2.33297  (2.33297)\n",
            "     | > loss_kl: 7.53367  (7.53367)\n",
            "     | > loss_feat: 4.19700  (4.19700)\n",
            "     | > loss_mel: 16.57518  (16.57518)\n",
            "     | > loss_duration: 2.40384  (2.40384)\n",
            "     | > loss_1: 33.04266  (33.04266)\n",
            "\n",
            " | > Synthesizing test sentences.\n",
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.35301 \u001b[0m(-0.17353)\n",
            "     | > avg_loss_disc:\u001b[92m 2.80673 \u001b[0m(-0.48032)\n",
            "     | > avg_loss_disc_real_0:\u001b[92m 0.57194 \u001b[0m(-0.15779)\n",
            "     | > avg_loss_disc_real_1:\u001b[91m 0.18746 \u001b[0m(+0.00244)\n",
            "     | > avg_loss_disc_real_2:\u001b[92m 0.17755 \u001b[0m(-0.05876)\n",
            "     | > avg_loss_disc_real_3:\u001b[92m 0.24213 \u001b[0m(-0.02564)\n",
            "     | > avg_loss_disc_real_4:\u001b[91m 0.23505 \u001b[0m(+0.00891)\n",
            "     | > avg_loss_disc_real_5:\u001b[91m 0.21293 \u001b[0m(+0.00085)\n",
            "     | > avg_loss_0:\u001b[92m 2.80673 \u001b[0m(-0.48032)\n",
            "     | > avg_loss_gen:\u001b[91m 2.33297 \u001b[0m(+0.22688)\n",
            "     | > avg_loss_kl:\u001b[91m 7.53367 \u001b[0m(+0.06533)\n",
            "     | > avg_loss_feat:\u001b[92m 4.19700 \u001b[0m(-0.01600)\n",
            "     | > avg_loss_mel:\u001b[91m 16.57518 \u001b[0m(+1.09954)\n",
            "     | > avg_loss_duration:\u001b[92m 2.40384 \u001b[0m(-0.06792)\n",
            "     | > avg_loss_1:\u001b[91m 33.04266 \u001b[0m(+1.30783)\n",
            "\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 343/100000\u001b[0m\n",
            " --> /content/drive/MyDrive/new-dataset/traineroutput/vits_ljspeech_ly-May-10-2023_06+03AM-0000000\n",
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: True\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: espeak\n",
            "| > Number of instances : 392\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 145\n",
            " | > Min text length: 8\n",
            " | > Avg text length: 53.558673469387756\n",
            " | \n",
            " | > Max audio length: 176422.0\n",
            " | > Min audio length: 17770.0\n",
            " | > Avg audio length: 67238.94897959183\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 32.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "\u001b[1m > TRAINING (2023-05-10 09:41:18) \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!CUDA_VISIBLE_DEVICES=\"0\" python /content/drive/MyDrive/$dataset_name/train_vits.py --restore_path /content/drive/MyDrive/$dataset_name/$output_directory/$run_name/$model_checkpoint --config_path /content/drive/MyDrive/$dataset_name/$run_name/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ2WoSCqzqxH"
      },
      "source": [
        "View Memory Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryma4sViztPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86eca0cf-b2f5-43ce-ba33-15fcbe13dd18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 10 06:03:19 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ORJUVXnz4B5"
      },
      "source": [
        "Load Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMqzKCacnPvX"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkt32dP3z9sK"
      },
      "source": [
        "Load Dashboard. May take several minutes to appear from a blank white box.  Ad blockers probably need to whitelist a bunch of Colab stuff or this won't wrok."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxPBHCGf0AVt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "outputId": "90ddd8a7-77cc-4e54-e55e-b5f5fbb54714"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/$dataset_name/$output_directory/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}